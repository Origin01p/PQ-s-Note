

从一个例子，来说明一个简单的理解。

> 调色盘问题，我们为了得到目的颜色，如何从既定的原色中调试出？
> 
> 要从已有的颜色（参数）进行调和（权重和偏差），我们为了接近目的色（标签数据），设置了一个损失函数，表示和预测和目的的差别，我们的期望当然是越小越好。而在这些调色这种，我们会发现，有一种颜色会使趋近速度最快，类似于数学概念上的梯度，所以这也是一种重要的调整函数。不断重复这个过程，我们得到了目的。尽管人不能理解这些细小累积的全部过程（黑箱），但是得到了目的。
> 
> 所以机器学习的重点便在于这个算法，也就是自我优化（学习）的这个过程。而算法的分类又和具体的目的有关。


以下是一些简要的基本脉络和导图，更详尽的在最后页的附录。

## 时间线

起始：1980s-1990（BP神经网络，决策树，CNN）
探索：1990s-2010（SVM，Boost，RNN，流形学习，随机森林，深度学习)
繁荣：2010至今（深度学习时代：NLP，CV，ASR）

## 根据输入数据的类型：

 结构化
 - 表格类
 - 时间序列数据

 [[非结构化]]：

- [[CV算法]]：机器视觉
- [[NLP算法]]：自然语言处理
- 语音识别
- 文本数据

## 模型架构分为

传统算法
 - 线性回归，逻辑回归，决策树，k近邻，朴素贝叶斯，支持向量机
 - k均值聚类，PCA降维
 
[[集成学习]]
 - Bagging（随机森林）
 - Boosting（Adaboost，GBDT，XGBoost ，LightGBM, CatBoost）
 - Stacking（基/元学习器）

神经网络（[[深度学习]]）
-  CNN，RNN，Transformer，生成对抗



## 根据学习范式：

- [[监督学习]]
	分类（离散），回归（连续）

- [[无监督学习]]
	聚类，降维（保留最本质的特征）

- [[强化学习]]
	策略，控制，优化


## 主要的机器学习算法

- [[基于逻辑回归的分类预测]]

- [[朴素贝叶斯]]

- [[K近邻]]

- [[支持向量机]]

- [[基于决策树的分类预测]]

- [[梯度提升树]]

- [[XGBoost]]

- [[LightGBM]]

- [[CatBoost]]



## 深度学习方面

##### 数据工程

- 数据收集/清洗：爬虫，数据标注，噪声处理
- 特征：归一化，特征选择

##### 核心网络架构

- [[卷积神经网络]]：CNN，适用于图像处理、检测、分割，人脸识别。

- [[循环神经网络]]:  RNN，适用于自然语言处理，时间序列预测，手写识别，语音识别（已被transformer冲击）

- [[Transformer]]：自注意力机制，广泛应用于大语言模型，同时对上述架构都有极大冲击。

- [[生成对抗网络]]：GAN

##### [[训练与优化算法]]

- 优化（ [[Adam优化]]）
 
- 正则化（[[Dropout]]）



### [[哲学思考]]：
我坚信，当发展到科学前沿的时候，面临无方向的迷茫时，决定未来的便是己身的哲学观：你怎么看待世界？
以我有限的见解来看，关于机器学习，本质仍是统计。那么世间万物，都是可以被统计预测出来的吗？我们的宇宙的本质逻辑是否是这样的超越人类心智结构的黑箱？

---------------


附录

#### 网格图

机器学习分类
├── 按学习范式分类
│   ├── 监督学习
│   │   ├── 分类问题
│   │   │   ├── 二元分类
│   │   │   ├── 多元分类
│   │   │   └── 多标签分类
│   │   └── 回归问题
│   │       ├── 线性回归
│   │       ├── 非线性回归
│   │       └── 时间序列预测
│   ├── 无监督学习
│   │   ├── 聚类
│   │   │   ├── 划分聚类（K-means）
│   │   │   ├── 层次聚类
│   │   │   ├── 密度聚类（DBSCAN）
│   │   │   └── 谱聚类
│   │   └── 降维
│   │       ├── 线性降维（PCA）
│   │       ├── 非线性降维（t-SNE, UMAP）
│   │       └── 流形学习
│   ├── 半监督学习
│   │   ├── 自训练
│   │   ├── 协同训练
│   │   └── 生成式方法
│   ├── 强化学习
│   │   ├── 基于值的方法（Q-learning）
│   │   ├── 基于策略的方法（Policy Gradient）
│   │   └── 演员-评论家方法（A2C, PPO）
│   └── 自监督学习
│       ├── 对比学习
│       ├── 掩码语言模型
│       └── 自编码器
├── 按模型架构分类
│   ├── 传统机器学习模型
│   │   ├── 线性模型
│   │   │   ├── 线性回归
│   │   │   ├── 逻辑回归
│   │   │   └── 感知机
│   │   ├── 基于树的模型
│   │   │   ├── 决策树
│   │   │   ├── 随机森林
│   │   │   └── 梯度提升树（XGBoost, LightGBM）
│   │   ├── 支持向量机
│   │   │   ├── 线性SVM
│   │   │   └── 核方法SVM
│   │   └── 贝叶斯模型
│   │       ├── 朴素贝叶斯
│   │       └── 贝叶斯网络
│   ├── 神经网络模型
│   │   ├── 前馈神经网络
│   │   │   ├── 多层感知机
│   │   │   └── 深度神经网络
│   │   ├── 卷积神经网络
│   │   │   ├── 图像分类网络（ResNet, VGG）
│   │   │   ├── 目标检测网络（YOLO, Faster R-CNN）
│   │   │   └── 语义分割网络（U-Net, DeepLab）
│   │   ├── 循环神经网络
│   │   │   ├── 简单RNN
│   │   │   ├── LSTM
│   │   │   └── GRU
│   │   ├── 注意力机制与Transformer
│   │   │   ├── Transformer编码器-解码器
│   │   │   ├── BERT（双向编码器）
│   │   │   └── GPT系列（自回归模型）
│   │   ├── 生成对抗网络
│   │   │   ├── 条件GAN
│   │   │   ├── StyleGAN
│   │   │   └── CycleGAN
│   │   └── 图神经网络
│   │       ├── GCN
│   │       ├── GAT
│   │       └── GraphSAGE
│   └── 集成学习方法
│       ├── Bagging（随机森林）
│       ├── Boosting（AdaBoost, Gradient Boosting）
│       └── Stacking
├── 按任务目标分类
│   ├── 预测任务
│   │   ├── 分类预测
│   │   ├── 回归预测
│   │   └── 异常检测
│   ├── 生成任务
│   │   ├── 文本生成
│   │   ├── 图像生成
│   │   ├── 音频生成
│   │   └── 数据增强
│   ├── 理解任务
│   │   ├── 自然语言理解
│   │   ├── 计算机视觉理解
│   │   └── 多模态理解
│   └── 决策任务
│       ├── 推荐系统
│       ├── 机器人控制
│       └── 游戏AI
├── 按数据特征分类
│   ├── 结构化数据学习
│   │   ├── 表格数据
│   │   └── 时间序列数据
│   ├── 非结构化数据学习
│   │   ├── 文本数据
│   │   ├── 图像数据
│   │   ├── 音频数据
│   │   └── 视频数据
│   └── 图结构数据学习
│       ├── 社交网络分析
│       ├── 分子结构分析
│       └── 知识图谱
└── 按训练方式分类
    ├── 批量学习
    ├── 在线学习
    ├── 增量学习
    ├── 迁移学习
    │   ├── 领域自适应
    │   └── 预训练+微调
    ├── 元学习
    │   ├── 基于优化的元学习
    │   ├── 基于模型的元学习
    │   └── 基于度量的元学习
    └── 联邦学习
        ├── 横向联邦学习
        ├── 纵向联邦学习
        └── 联邦迁移学习


### 时间线

1980s：传统统计方法主导
1990s：SVM、集成方法兴起
2000s：浅层神经网络、特征工程
2012：深度学习突破（AlexNet）
2014：GAN提出、注意力机制
2017：Transformer架构
2018：BERT/GPT预训练模型
2020：大模型时代开启
2023：多模态大模型爆发
