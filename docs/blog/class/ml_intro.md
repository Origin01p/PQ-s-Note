---
title: 机器学习导论
---

# 机器学习导论

我们从一个简单的调色盘例子来入手，问题：为得到目的色，该如何从既定原色中调试出？

过程很自然：要从已有的颜色（参数）进行调和（权重和偏差），我们为了接近目的色（标签数据），设置了一个损失函数，表示和预测和目的的差别，我们的期望当然是越小越好。

在这些调色这种，我们会发现，有一种颜色会使趋近速度最快，类似于数学概念上的梯度（这被视为一种重要的调整函数）。不断重复这个过程我们得到了我们想要的颜色，尽管人不能理解这些细小累积的全部过程（黑箱）。
 
 很显然机器学习的重点在于这个算法，也就是自我优化（学习）的这个过程。而算法的分类又和具体的目的有关。


以下是一些简要的基本脉络和导图，更详尽的在最后页的附录。

## 时间线

起始：1980s - 1990（BP神经网络，决策树，CNN）
探索：1990s - 2010（SVM，Boost，RNN，LSTM，流形学习，随机森林，深度学习）
繁荣：2010s - 至今（深度学习时代：NLP，CV，GAN，AGI）

## 根据输入数据的类型

 结构化数据
 
 - 表格类
 - 时间序列数据

 非结构化数据
 
- 机器视觉
- 自然语言处理
- 语音识别
- 图数据

## 根据模型架构

统计学习

 - [[线性回归]]、[[逻辑回归]]、[[感知机]]、[[k近邻]]、[[支持向量机]]、[[最大熵模型]]、[[决策树]]
 - [[朴素贝叶斯]]、 [[隐马尔可夫模型]]、[[条件随机场]]
 - [[k-Means]]、[[PCA降维]]、[[LDA降维]]、[[EM算法]]、[[GMM]]
 
[[集成学习]]

 - Bagging：[[随机森林]]
 - Boosting：Adaboost、[[GBDT]]、[[XGBoost]] 、[[LightGBM]]、 CatBoost
 - Stacking（基/元学习器）

神经网络（[[深度学习]]）

- [[卷积神经网络]]（CNN）、[[循环神经网络]]（RNN）、[[LSTM]]、[[Transformer]]、[[多层感知机]]
- [[生成对抗网络]]（GAN）、[[扩散模型]]（DIffusion）


## 根据学习范式

#### [[监督学习]]

- 分类 （离散）
     - 线性模型：[[逻辑回归]]、[[最大熵模型]]
     - 经典模型：[[支持向量机]]、[[决策树]]、[[随机森林]]、[[XGBoost]]。
     - 深度学习：MLN、CNN、RNN/LSTM（序列）
 - 回归（连续）
     - 线性回归、[[岭回归]]
     - 神经网络回归。

#### [[无监督学习]]（/自监督）

- 聚类 
    - k-Means
    - GMM（高斯混合模型，使用 [[EM算法]]训练）
- 降维 
    - PCA降维
    - t-SNE
- 生成模型（可视为一个特殊交集）
    - **GAN**、**VAE**、**扩散模型**、**Transformer (GPT)**。
    - _注：它们虽然有时使用数据本身做“自监督”，但本质上是在学概率分布。_

#### [[强化学习]]

- 策略，控制，优化
- **基础算法**：Q-Learning、SARSA。
- **深度强化学习 (Deep RL)**：把深度学习作为“大脑”来处理复杂输入。
    - DQN (Deep Q Network)。
    - Policy Gradient。
    - [[PPO]] (目前大模型 RLHF 阶段最常用的算法)。


## 主要的机器学习算法

- [[基于逻辑回归的分类预测]]

- [[朴素贝叶斯]]

- [[K近邻]]

- [[支持向量机]]

- [[基于决策树的分类预测]]

- [[GBDT]]

- [[XGBoost]]

- [[LightGBM]]

- [[CatBoost]]



### [[哲学思考]]：
我坚信，当发展到科学前沿的时候，面临无方向的迷茫时，决定未来的便是己身的哲学观：你怎么看待世界？
以我有限的见解来看，关于机器学习，本质仍是统计。那么世间万物，都是可以被统计预测出来的吗？我们的宇宙的本质逻辑是否是这样的超越人类心智结构的黑箱？

