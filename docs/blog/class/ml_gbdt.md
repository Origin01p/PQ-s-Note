---
title: 梯度提升决策树
---

# GBDT

- 归属：集成学习中的Boosting。（进阶也是常用：XGboost）
- 过程：利用多个决策树（通常是CART树）逐步拟合数据，新加入的树在前一棵树的残差基础上学习。
- 核心思想：梯度下降。将损失函数的负梯度作为残差的近似，从而序列化构建每一颗决策树，将所有树的预测结果加权得到集成模型。
- 优点：可解释性强、预测精度高
- 缺点：计算复杂度高、串行生成故并行难

## 重要参数

- n_estimators：树的数量（迭代次数），太少会欠拟合，太多会过拟合且计算慢。通常配合早停法使用。
- learning_rate：学习率（每棵树的贡献权重）。
- max_depth：树的最大深度。
- max_leaf_nodes：最大叶节点数。

