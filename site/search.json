{"config":{"separator":"[\\s\\u200b\\-]"},"items":[{"location":"","level":1,"title":"个人","text":"<p>For full documentation visit zensical.org.</p>","path":["个人"],"tags":[]},{"location":"#commands","level":2,"title":"Commands","text":"<ul> <li><code>zensical new</code> - Create a new project</li> <li><code>zensical serve</code> - Start local web server</li> <li><code>zensical build</code> - Build your site</li> </ul>","path":["个人"],"tags":[]},{"location":"#examples","level":2,"title":"Examples","text":"","path":["个人"],"tags":[]},{"location":"#admonitions","level":3,"title":"Admonitions","text":"<p>Go to documentation</p> <p>Note</p> <p>This is a note admonition. Use it to provide helpful information.</p> <p>Warning</p> <p>This is a warning admonition. Be careful!</p>","path":["个人"],"tags":[]},{"location":"#details","level":3,"title":"Details","text":"<p>Go to documentation</p> Click to expand for more info <p>This content is hidden until you click to expand it. Great for FAQs or long explanations.</p>","path":["个人"],"tags":[]},{"location":"#code-blocks","level":2,"title":"Code Blocks","text":"<p>Go to documentation</p> Code blocks<pre><code>def greet(name):\n    print(f\"Hello, {name}!\") # (1)!\n\ngreet(\"Python\")\n</code></pre> <ol> <li> <p>Go to documentation</p> <p>Code annotations allow to attach notes to lines of code.</p> </li> </ol> <p>Code can also be highlighted inline: <code>print(\"Hello, Python!\")</code>.</p>","path":["个人"],"tags":[]},{"location":"#content-tabs","level":2,"title":"Content tabs","text":"<p>Go to documentation</p> PythonRust <pre><code>print(\"Hello from Python!\")\n</code></pre> <pre><code>println!(\"Hello from Rust!\");\n</code></pre>","path":["个人"],"tags":[]},{"location":"#diagrams","level":2,"title":"Diagrams","text":"<p>Go to documentation</p> <pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];\n</code></pre>","path":["个人"],"tags":[]},{"location":"#footnotes","level":2,"title":"Footnotes","text":"<p>Go to documentation</p> <p>Here's a sentence with a footnote.<sup>1</sup></p> <p>Hover it, to see a tooltip.</p>","path":["个人"],"tags":[]},{"location":"#formatting","level":2,"title":"Formatting","text":"<p>Go to documentation</p> <ul> <li>This was marked (highlight)</li> <li>This was inserted (underline)</li> <li>This was deleted (strikethrough)</li> <li>H<sub>2</sub>O</li> <li>A<sup>T</sup>A</li> <li>Ctrl+Alt+Del</li> </ul>","path":["个人"],"tags":[]},{"location":"#icons-emojis","level":2,"title":"Icons, Emojis","text":"<p>Go to documentation</p> <ul> <li> <code>:sparkles:</code></li> <li> <code>:rocket:</code></li> <li> <code>:tada:</code></li> <li> <code>:memo:</code></li> <li> <code>:eyes:</code></li> </ul>","path":["个人"],"tags":[]},{"location":"#maths","level":2,"title":"Maths","text":"<p>Go to documentation</p> \\[ \\cos x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{(2k)!}x^{2k} \\] <p>Needs configuration</p> <p>Note that MathJax is included via a <code>script</code> tag on this page and is not configured in the generated default configuration to avoid including it in a pages that do not need it. See the documentation for details on how to configure it on all your pages if they are more Maths-heavy than these simple starter pages.</p>","path":["个人"],"tags":[]},{"location":"#task-lists","level":2,"title":"Task Lists","text":"<p>Go to documentation</p> <ul> <li> Install Zensical</li> <li> Configure <code>zensical.toml</code></li> <li> Write amazing documentation</li> <li> Deploy anywhere</li> </ul>","path":["个人"],"tags":[]},{"location":"#tooltips","level":2,"title":"Tooltips","text":"<p>Go to documentation</p> <p>Hover me</p> <ol> <li> <p>This is the footnote. ↩</p> </li> </ol>","path":["个人"],"tags":[]},{"location":"about/","level":1,"title":"关于 Zensical-Wcowin","text":"","path":["关于"],"tags":[]},{"location":"about/#_1","level":2,"title":"项目简介","text":"<p>Zensical-Wcowin 是最详细、最便捷、最前沿的 Zensical 中文教程。</p>","path":["关于"],"tags":[]},{"location":"about/#_2","level":2,"title":"作者","text":"<p>Wcowin</p> <ul> <li>GitHub: @Wcowin</li> <li>Email: wcowin@qq.com</li> <li>Telegram: @Wcowin</li> </ul>","path":["关于"],"tags":[]},{"location":"about/#_3","level":2,"title":"贡献","text":"<p>欢迎参与项目完善！</p>","path":["关于"],"tags":[]},{"location":"about/#license","level":2,"title":"License","text":"<p>MIT License</p>","path":["关于"],"tags":[]},{"location":"blog/posts/2025-01-22-hello-world/","level":1,"title":"我的第一篇博客","text":"<p>这是文章内容...</p>","path":["Blog","Posts","我的第一篇博客"],"tags":["Zensical","教程"]},{"location":"class/Probability/chapter1/","level":1,"title":"Chapter1","text":"<p>研究对象：随机现象 概率论研究：随机现象模型（概率分布） 数理统计研究：随机现象的数据收集，处理，统计推断</p>","path":["Class","Probability","Chapter1"],"tags":[]},{"location":"class/Probability/chapter1/#1","level":4,"title":"1.随机现象","text":"<p>分类：随机现象，确定性现象</p> <p>侧重：概统主要注重能大量重复的随机现象，也关注不能重复的</p>","path":["Class","Probability","Chapter1"],"tags":[]},{"location":"class/Probability/chapter1/#11","level":6,"title":"1.1 术语：","text":"<p>样本空间：一切可能的结果的集合，其中元素称作样本点。 - 至少有两个。有一个的称为确定性现象。 - 有限或可列个称为离散样本空间，不可列无限个才成为连续样本空间。</p> <p>随机事件（事件）：某些样本点组成的子集。 - 常用Venn图表示 - 单个元素组成的称为基本事件 - 最大子集即本身称为必然事件，最小子集即空集称为不可能事件。</p> <p>随机变量：表示随机现象结果的变量 - 常用大写字母表示 - 应当认为注明含义</p>","path":["Class","Probability","Chapter1"],"tags":[]},{"location":"class/Probability/chapter1/#12","level":6,"title":"1.2 关系及运算：","text":"<p>事件间的关系 - 包含关系 - 相等关系 - 互不相容</p> <p>事件间的运算 - 并:\\(A\\bigcup B\\) - 差：\\(A-B\\) - 对立：\\(\\bar{A}\\)</p> <p>运算规则： - 交换律，结合律，分配律，对偶律（德摩根公式）</p>","path":["Class","Probability","Chapter1"],"tags":[]},{"location":"class/Probability/chapter1/#13","level":6,"title":"1.3 引申事件域：","text":"<p>事件域 - 子集经过运算生成的集合类 - \\(\\sigma\\)代数 - 常见的：Borel域（可测集）</p> <p>分割 - 事件互不相容，可以构成一个分割</p>","path":["Class","Probability","Chapter1"],"tags":[]},{"location":"class/Probability/chapter1/#2","level":4,"title":"2. 概率","text":"","path":["Class","Probability","Chapter1"],"tags":[]},{"location":"class/Probability/chapter1/#21","level":5,"title":"2.1 概率的定义","text":"<p>发展脉络： - 历史发展：古典定义，几何定义，频率定义，主观定义 - 公理化定义（Kolmogorov，苏联）</p> <p>公理化定义(Kolmogorov) - 非负性：大于等于0 - 正则性：整个样本空间概率为1 - 可列可加性 满足上述条件的，则称P(A)为概率</p> <p>排列与组合公式 - 乘法原理 - 加法原理</p>","path":["Class","Probability","Chapter1"],"tags":[]},{"location":"class/Probability/chapter1/#22","level":5,"title":"2.2 确定概率的方法：","text":"<p>频率方法确定概率 - 大量重复实验统计频率</p> <p>古典方法确定概率 - 在有限样本的随机现象中，每个样本等可能 - 抽样模型：彩票问题 - 盒子模型：生日问题，统计物理</p> <p>几何方法确定概率 - 会面问题 - 比丰投针（蒙塔卡罗方法）</p> <p>主观方法确定概率 - 贝叶斯学派：概率为根据经验做出的个人可能性判断 - 气象预报，经验总体预估</p>","path":["Class","Probability","Chapter1"],"tags":[]},{"location":"class/Probability/chapter1/#3","level":4,"title":"3.概率的性质","text":"<p>可加性</p> <p>单调性</p> <p>加法公式</p> <p>连续性</p>","path":["Class","Probability","Chapter1"],"tags":[]},{"location":"class/Probability/chapter1/#4","level":4,"title":"4.条件概率","text":"<p>条件概率，在事件B发生的条件下，A发生的概率，记为\\(P(A|B)\\)</p> <p>条件概率的实用公式 - 乘法公式 - 全概率公式 - 贝叶斯公式</p>","path":["Class","Probability","Chapter1"],"tags":[]},{"location":"class/Probability/chapter1/#5","level":5,"title":"5.独立性问题","text":"","path":["Class","Probability","Chapter1"],"tags":[]},{"location":"class/Probability/chapter2/","level":1,"title":"Chapter2","text":"<p>随机变量及其分布是概率论的重点，本章主要讨论一维随机变量及其分布。</p>","path":["Class","Probability","Chapter2"],"tags":[]},{"location":"class/Probability/chapter2/#1","level":4,"title":"1. 随机变量及其分布函数","text":"<p>随机变量：定义在样本空间上的实值函数，常用大写字母表示，取值常用小写字母表示。</p> <p>分布函数：对于一个随机变量，定义\\(F(x)=P(X\\leq x)\\)为分布函数，称X服从\\(F(x)\\).满足下列三种基本性质： - 单调性 - 有界性 - 右连续性</p> <p>离散随机变量：分布列（非负性，正则性）</p> <p>连续随机变量：密度函数（非负性，正则性）</p>","path":["Class","Probability","Chapter2"],"tags":[]},{"location":"class/Probability/chapter2/#2","level":4,"title":"2. 随机变量的数学期望","text":"<p>根据分布计算随机变量的特征数（均值，方差，分位数），侧面反映分布的特征。下介绍最重要的特征数：数学期望。</p> <p>起源：分赌本问题 均值：算术平均，加权平均（根据频率） 离散表达式：\\(E(x)=\\sum x_{i}p(x_{i})\\)   (当且仅当绝对收敛的时候，此时唯一) 连续表达式：\\(E(s)=\\int xp(x) \\, dx\\) 物理解释：重心 统计学作用：消除随机性的重要手段</p> <p>数学期望的性质： \\(E(c)=c\\) \\(E(aX)=aE(x)\\) \\(E(g_{1}+g_{2})=E(g_{1})+E(g_{2})\\)</p>","path":["Class","Probability","Chapter2"],"tags":[]},{"location":"class/Probability/chapter2/#3","level":4,"title":"3. 随机变量的方差与标准差","text":"<p>方差： 对于随机变量\\(X\\)的均值a，定义新的随机变量\\((x-a)^2\\)，其均值称作方差。 \\(Var(X)=E(x-E(x))^2\\)</p> <p>标准差： 定义方差的平方根：\\(\\sqrt{ Var(x) }=\\sigma(X)\\)</p> <p>作用：都刻画\\(X\\)的波动程度（集中与分散程度）。方差/标准差越小，取值愈集中。</p> <p>注：由于标准差的量纲与数学期望一致，标准差的使用倾向更大。期望存在，方差不一定存在。但方差存在，期望一定存在。</p> <p>性质*： 1.\\(Var(X)=E(X^2)-[E(x)]^2\\) 2.常数的方差为0 3.\\(Var(aX+b)=a^2Var(X)\\) 4.切比雪夫不等式（chebyshev）： $$ P(\\mid X-E(x)|\\geq \\sigma) \\leq \\frac{Var(X)}{\\sigma^2} $$  对于切比雪夫不等式，描述了这样一个现实：  大偏差（\\(X-E(x)\\)）的概率，称作偏差发生概率，其上界与方差成正比，方差愈大，其上界愈大。</p>","path":["Class","Probability","Chapter2"],"tags":[]},{"location":"class/Probability/chapter2/#4","level":4,"title":"4. 常用的几种分布（离散）","text":"<p>二项分布   定义：描述在 \\( n \\) 次独立重复的伯努利试验中，成功次数 \\( X \\) 的概率分布。   概率质量函数$\\(P(X=k) = C_n^k p^k (1-p)^{n-k}, \\quad k=0,1,\\dots,n\\)$期望与方差：  $\\(E(X)=np,\\quad Var(X)=np(1-p)\\)$应用：适用于结果只有两种（成功/失败）的重复独立试验。</p> <p>泊松分布   定义：描述在单位时间（或空间）内，随机事件发生次数的概率分布。   概率质量函数：  $$   P(X=k)=\\frac{\\lambda^k e^{-\\lambda}}{k!}, \\quad k=0,1,2,\\dots$\\(**期望与方差**：\\)\\(E(X)=\\lambda,\\quad Var(X)=\\lambda\\)$应用：稀有事在一定时间或区间内发生次数的建模。</p> <p>关系：泊松分布可以作为二项分布的近似（泊松定理：当 \\(n\\) 很大、\\(p\\)  很小，且 \\(np\\) 适中时，泊松分布可作为二项分布的近似 \\(\\lambda = np\\) ）</p> <p>超几何分布  定义：描述在不放回抽样中，从包含 \\( M \\) 个成功个体和 \\( N-M \\) 个失败个体的总体中抽取 \\( n \\) 个个体，其中成功个体数 \\( X \\) 的分布。  概率质量函数：  $$   P(X=k)=\\frac{CM^k C<sup>{n-k}}{C_N</sup>n}$$期望与方差：  $$   E(X)=n\\cdot\\frac{M}{N},\\quad Var(X)=n\\cdot\\frac{M}{N}\\cdot\\left(1-\\frac{M}{N}\\right)\\cdot\\frac{N-n}{N-1}$$应用：适用于不放回抽样的质量控制或抽样调查。</p> <p>几何分布   定义：描述在多次独立伯努利试验中，首次成功所需试验次数 \\( X \\) 的概率分布。   概率质量函数：  $\\(P(X=k)=(1-p)^{k-1}p,\\quad k=1,2,\\dots\\)$期望与方差：  $$  E(X)=\\frac{1}{p},\\quad Var(X)=\\frac{1-p}{p^2}$$应用：用于建模在多次尝试中首次获得成功所需的次数。</p> <p>负二项分布   定义：描述在独立重复试验中，获得第 \\( r \\) 次成功所需试验次数 \\( X \\) 的概率分布。   概率质量函数：  $\\(P(X=k)=C_{k-1}^{r-1}p^r(1-p)^{k-r},\\quad k=r,r+1,\\dots\\)\\(**期望与方差**：\\)\\(E(X)=\\frac{r}{p},\\quad Var(X)=\\frac{r(1-p)}{p^2}\\)$应用：适用于需要多次成功的情境，如多次试验直到达成特定目标。</p>","path":["Class","Probability","Chapter2"],"tags":[]},{"location":"class/Probability/chapter2/#5","level":4,"title":"5. 常用的几种分布（连续）","text":"<p>正态分布   定义：又称高斯分布，是连续型概率分布中最重要的一种，由均值 \\(\\mu\\) 和方差 \\(\\sigma^2\\)决定。   概率密度函数：  $\\(f(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}},\\quad -\\infty&lt;x&lt;\\infty\\)$期望与方差：  $\\(E(X)=\\mu,\\quad Var(X)=\\sigma^2\\)$ 应用：广泛应用于自然科学和社会科学中的数据建模。</p> <p>均匀分布   定义：在区间 \\([a,b]\\) 内，每个点出现的概率相等的分布。   概率密度函数：  $$f(x)=\\begin{cases}   \\frac{1}{b-a}, &amp; a\\le x\\le b \\   0, &amp; \\text{otherwise}   \\end{cases} $$期望与方差：  $$   E(X)=\\frac{a+b}{2},\\quad Var(X)=\\frac{(b-a)^2}{12}$$应用：适用于在某个区间内等可能取值的随机现象。</p> <p>指数分布   定义：描述泊松过程中事件发生时间间隔的概率分布。   概率密度函数：  $\\(f(x)=\\lambda e^{-\\lambda x},\\quad x\\ge0\\)$期望与方差：  $\\(E(X)=\\frac{1}{\\lambda},\\quad Var(X)=\\frac{1}{\\lambda^2}\\)$应用：常用于可靠性分析和排队论，描述无记忆性的随机时间间隔。</p> <p>伽马分布   定义：指数分布的推广，描述多个独立指数分布随机变量之和的分布。   概率密度函数：  $$ f(x)=\\frac{\\beta<sup>\\alpha}{\\Gamma(\\alpha)}x</sup>,\\quad x&gt;0$$}e^{-\\beta x期望与方差：  $$   E(X)=\\frac{\\alpha}{\\beta},\\quad Var(X)=\\frac{\\alpha}{\\beta^2} $$应用：适用于对连续正随机变量建模，如等待时间的总和。</p> <p>贝塔分布   定义：定义在区间 \\([0,1]\\) 上的连续概率分布，由两个正参数 \\(\\alpha\\) 和 \\(\\beta\\)  决定。   概率密度函数：  $\\(f(x)=\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha,\\beta)},\\quad 0\\le x\\le1\\)$期望与方差：  $\\(E(X)=\\frac{\\alpha}{\\alpha+\\beta},\\quad Var(X)=\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\)$应用：常用于表示概率的概率分布，适用于比例或百分比的建模。</p> <p>卡方分布   定义：是 k 个独立标准正态随机变量的平方和所服从的分布，其中k为自由度。   概率密度函数：  $\\(f(x)=\\frac{1}{2^{k/2}\\Gamma(k/2)}x^{k/2-1}e^{-x/2},\\quad x&gt;0\\)$期望与方差：  $$ E(X)=k,\\quad Var(X)=2k$$应用：主要用于假设检验和置信区间的构建，特别是方差分析和拟合优度检验。</p>","path":["Class","Probability","Chapter2"],"tags":[]},{"location":"class/Probability/chapter2/#6","level":4,"title":"6. 分布的其他特征数","text":"<p>\\(k\\)阶原点矩 - \\(\\mu_{k}= E(X^k)\\) - 一阶原点矩：数学期望</p> <p>\\(k\\)阶中心矩 - \\(\\nu_{k}= E(X-E(X))^k\\) - 二阶中心矩：方差</p> <p>二者存在对应关系</p> <p>变异系数：标准差比期望（消除量纲的影响）</p> <p>分位数：分为两块（下侧，上侧）</p> <p>中位数：均分的分位数</p> <p>偏度系数：描述分布偏离对称性的程度（三阶中心矩/二阶中心矩的3/2次方）</p> <p>峰度系数：描述分布尖峭程度或尾部粗细（四阶中心矩/二阶中心矩的2次方 -3）</p>","path":["Class","Probability","Chapter2"],"tags":[]},{"location":"class/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/","level":1,"title":"机器学习导论","text":"<p>从一个例子，来说明一个简单的理解。</p> <p>调色盘问题，我们为了得到目的颜色，如何从既定的原色中调试出？</p> <p>要从已有的颜色（参数）进行调和（权重和偏差），我们为了接近目的色（标签数据），设置了一个损失函数，表示和预测和目的的差别，我们的期望当然是越小越好。而在这些调色这种，我们会发现，有一种颜色会使趋近速度最快，类似于数学概念上的梯度，所以这也是一种重要的调整函数。不断重复这个过程，我们得到了目的。尽管人不能理解这些细小累积的全部过程（黑箱），但是得到了目的。</p> <p>所以机器学习的重点便在于这个算法，也就是自我优化（学习）的这个过程。而算法的分类又和具体的目的有关。</p> <p>以下是一些简要的基本脉络和导图，更详尽的在最后页的附录。</p>","path":["Class","Ml","机器学习导论"],"tags":[]},{"location":"class/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/#_1","level":2,"title":"时间线","text":"<p>起始：1980s-1990（BP神经网络，决策树，CNN） 探索：1990s-2010（SVM，Boost，RNN，流形学习，随机森林，深度学习) 繁荣：2010至今（深度学习时代：NLP，CV，ASR）</p>","path":["Class","Ml","机器学习导论"],"tags":[]},{"location":"class/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/#_2","level":2,"title":"根据输入数据的类型：","text":"<p>结构化  - 表格类  - 时间序列数据</p> <p>[[非结构化]]：</p> <ul> <li>[[CV算法]]：机器视觉</li> <li>[[NLP算法]]：自然语言处理</li> <li>语音识别</li> <li>文本数据</li> </ul>","path":["Class","Ml","机器学习导论"],"tags":[]},{"location":"class/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/#_3","level":2,"title":"模型架构分为","text":"<p>传统算法  - 线性回归，逻辑回归，决策树，k近邻，朴素贝叶斯，支持向量机  - k均值聚类，PCA降维</p> <p>[[集成学习]]  - Bagging（随机森林）  - Boosting（Adaboost，GBDT，XGBoost ，LightGBM, CatBoost）  - Stacking（基/元学习器）</p> <p>神经网络（[[深度学习]]） -  CNN，RNN，Transformer，生成对抗</p>","path":["Class","Ml","机器学习导论"],"tags":[]},{"location":"class/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/#_4","level":2,"title":"根据学习范式：","text":"<ul> <li> <p>[[监督学习]]     分类（离散），回归（连续）</p> </li> <li> <p>[[无监督学习]]     聚类，降维（保留最本质的特征）</p> </li> <li> <p>[[强化学习]]     策略，控制，优化</p> </li> </ul>","path":["Class","Ml","机器学习导论"],"tags":[]},{"location":"class/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/#_5","level":2,"title":"主要的机器学习算法","text":"<ul> <li> <p>[[基于逻辑回归的分类预测]]</p> </li> <li> <p>[[朴素贝叶斯]]</p> </li> <li> <p>[[K近邻]]</p> </li> <li> <p>[[支持向量机]]</p> </li> <li> <p>[[基于决策树的分类预测]]</p> </li> <li> <p>[[梯度提升树]]</p> </li> <li> <p>[[XGBoost]]</p> </li> <li> <p>[[LightGBM]]</p> </li> <li> <p>[[CatBoost]]</p> </li> </ul>","path":["Class","Ml","机器学习导论"],"tags":[]},{"location":"class/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/#_6","level":2,"title":"深度学习方面","text":"","path":["Class","Ml","机器学习导论"],"tags":[]},{"location":"class/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/#_7","level":5,"title":"数据工程","text":"<ul> <li>数据收集/清洗：爬虫，数据标注，噪声处理</li> <li>特征：归一化，特征选择</li> </ul>","path":["Class","Ml","机器学习导论"],"tags":[]},{"location":"class/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/#_8","level":5,"title":"核心网络架构","text":"<ul> <li> <p>[[卷积神经网络]]：CNN，适用于图像处理、检测、分割，人脸识别。</p> </li> <li> <p>[[循环神经网络]]:  RNN，适用于自然语言处理，时间序列预测，手写识别，语音识别（已被transformer冲击）</p> </li> <li> <p>[[Transformer]]：自注意力机制，广泛应用于大语言模型，同时对上述架构都有极大冲击。</p> </li> <li> <p>[[生成对抗网络]]：GAN</p> </li> </ul>","path":["Class","Ml","机器学习导论"],"tags":[]},{"location":"class/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/#_9","level":5,"title":"[[训练与优化算法]]","text":"<ul> <li> <p>优化（ [[Adam优化]]）</p> </li> <li> <p>正则化（[[Dropout]]）</p> </li> </ul>","path":["Class","Ml","机器学习导论"],"tags":[]},{"location":"class/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/#_10","level":3,"title":"[[哲学思考]]：","text":"<p>我坚信，当发展到科学前沿的时候，面临无方向的迷茫时，决定未来的便是己身的哲学观：你怎么看待世界？ 以我有限的见解来看，关于机器学习，本质仍是统计。那么世间万物，都是可以被统计预测出来的吗？我们的宇宙的本质逻辑是否是这样的超越人类心智结构的黑箱？</p> <p>附录</p>","path":["Class","Ml","机器学习导论"],"tags":[]},{"location":"class/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/#_11","level":4,"title":"网格图","text":"<p>机器学习分类 ├── 按学习范式分类 │   ├── 监督学习 │   │   ├── 分类问题 │   │   │   ├── 二元分类 │   │   │   ├── 多元分类 │   │   │   └── 多标签分类 │   │   └── 回归问题 │   │       ├── 线性回归 │   │       ├── 非线性回归 │   │       └── 时间序列预测 │   ├── 无监督学习 │   │   ├── 聚类 │   │   │   ├── 划分聚类（K-means） │   │   │   ├── 层次聚类 │   │   │   ├── 密度聚类（DBSCAN） │   │   │   └── 谱聚类 │   │   └── 降维 │   │       ├── 线性降维（PCA） │   │       ├── 非线性降维（t-SNE, UMAP） │   │       └── 流形学习 │   ├── 半监督学习 │   │   ├── 自训练 │   │   ├── 协同训练 │   │   └── 生成式方法 │   ├── 强化学习 │   │   ├── 基于值的方法（Q-learning） │   │   ├── 基于策略的方法（Policy Gradient） │   │   └── 演员-评论家方法（A2C, PPO） │   └── 自监督学习 │       ├── 对比学习 │       ├── 掩码语言模型 │       └── 自编码器 ├── 按模型架构分类 │   ├── 传统机器学习模型 │   │   ├── 线性模型 │   │   │   ├── 线性回归 │   │   │   ├── 逻辑回归 │   │   │   └── 感知机 │   │   ├── 基于树的模型 │   │   │   ├── 决策树 │   │   │   ├── 随机森林 │   │   │   └── 梯度提升树（XGBoost, LightGBM） │   │   ├── 支持向量机 │   │   │   ├── 线性SVM │   │   │   └── 核方法SVM │   │   └── 贝叶斯模型 │   │       ├── 朴素贝叶斯 │   │       └── 贝叶斯网络 │   ├── 神经网络模型 │   │   ├── 前馈神经网络 │   │   │   ├── 多层感知机 │   │   │   └── 深度神经网络 │   │   ├── 卷积神经网络 │   │   │   ├── 图像分类网络（ResNet, VGG） │   │   │   ├── 目标检测网络（YOLO, Faster R-CNN） │   │   │   └── 语义分割网络（U-Net, DeepLab） │   │   ├── 循环神经网络 │   │   │   ├── 简单RNN │   │   │   ├── LSTM │   │   │   └── GRU │   │   ├── 注意力机制与Transformer │   │   │   ├── Transformer编码器-解码器 │   │   │   ├── BERT（双向编码器） │   │   │   └── GPT系列（自回归模型） │   │   ├── 生成对抗网络 │   │   │   ├── 条件GAN │   │   │   ├── StyleGAN │   │   │   └── CycleGAN │   │   └── 图神经网络 │   │       ├── GCN │   │       ├── GAT │   │       └── GraphSAGE │   └── 集成学习方法 │       ├── Bagging（随机森林） │       ├── Boosting（AdaBoost, Gradient Boosting） │       └── Stacking ├── 按任务目标分类 │   ├── 预测任务 │   │   ├── 分类预测 │   │   ├── 回归预测 │   │   └── 异常检测 │   ├── 生成任务 │   │   ├── 文本生成 │   │   ├── 图像生成 │   │   ├── 音频生成 │   │   └── 数据增强 │   ├── 理解任务 │   │   ├── 自然语言理解 │   │   ├── 计算机视觉理解 │   │   └── 多模态理解 │   └── 决策任务 │       ├── 推荐系统 │       ├── 机器人控制 │       └── 游戏AI ├── 按数据特征分类 │   ├── 结构化数据学习 │   │   ├── 表格数据 │   │   └── 时间序列数据 │   ├── 非结构化数据学习 │   │   ├── 文本数据 │   │   ├── 图像数据 │   │   ├── 音频数据 │   │   └── 视频数据 │   └── 图结构数据学习 │       ├── 社交网络分析 │       ├── 分子结构分析 │       └── 知识图谱 └── 按训练方式分类     ├── 批量学习     ├── 在线学习     ├── 增量学习     ├── 迁移学习     │   ├── 领域自适应     │   └── 预训练+微调     ├── 元学习     │   ├── 基于优化的元学习     │   ├── 基于模型的元学习     │   └── 基于度量的元学习     └── 联邦学习         ├── 横向联邦学习         ├── 纵向联邦学习         └── 联邦迁移学习</p>","path":["Class","Ml","机器学习导论"],"tags":[]},{"location":"class/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/#_12","level":3,"title":"时间线","text":"<p>1980s：传统统计方法主导 1990s：SVM、集成方法兴起 2000s：浅层神经网络、特征工程 2012：深度学习突破（AlexNet） 2014：GAN提出、注意力机制 2017：Transformer架构 2018：BERT/GPT预训练模型 2020：大模型时代开启 2023：多模态大模型爆发</p>","path":["Class","Ml","机器学习导论"],"tags":[]}]}