{"config":{"separator":"[\\s\\u200b\\-]"},"items":[{"location":"","level":1,"title":"PQ's Blog","text":"Hello,World! |          Github                 Email me        星火世传 奋飞不辍 加载中... <ul> <li> <p> 推荐的文章</p> </li> <li> <p> 一些课程</p> <ul> <li>概率论与统计</li> <li>实变函数与泛函分析</li> <li>复变函数</li> </ul> <ul> <li>机器学习(新) →</li> </ul> </li> <li> <p> 整理/汇总</p> <ul> <li>AI网站</li> <li>一些数学家的博客</li> <li>书目汇总</li> <li>参与过的项目或研讨会 </li> </ul> </li> <li> <p> 关于</p> <ul> <li>留言板<sup>1</sup></li> <li>博客</li> <li> 关于我</li> </ul> </li> </ul> <ol> <li> <p>梦醒人间看微雨，江山还似旧温柔 ↩</p> </li> </ol>","path":["PQ's Blog"],"tags":[]},{"location":"about/","level":1,"title":"关于我","text":"<p>本网站旨在个人部分学习资料的保存与展示。</p>","path":["关于"],"tags":[]},{"location":"about/#education","level":3,"title":"Education","text":"<ul> <li>2021.09 - 2025.06 中南大学，数学与统计学院</li> </ul>","path":["关于"],"tags":[]},{"location":"about/#experience","level":3,"title":"Experience","text":"<ul> <li>2026.01 — 至今 麻省理工学院（MIT）微硕士项目：统计学与数据科学（MicroMasters® Program in Statistics and Data Science）</li> </ul>","path":["关于"],"tags":[]},{"location":"about/#others","level":3,"title":"Others","text":"<ul> <li>Email: origin01p@outlook.com / pengwq7@gmail.com(备)</li> <li>@Github主页</li> <li>@Kaggle主页</li> </ul>","path":["关于"],"tags":[]},{"location":"about/#thanks","level":2,"title":"Thanks","text":"<p>感谢@Wcowin的技术支持！</p>","path":["关于"],"tags":[]},{"location":"archive/","level":1,"title":"归档","text":"<p>历年笔记</p>","path":["档案","归档"],"tags":[]},{"location":"techhelp/","level":1,"title":"个人","text":"<p>For full documentation visit zensical.org.</p>","path":["技术相关"],"tags":[]},{"location":"techhelp/#commands","level":2,"title":"Commands","text":"<ul> <li><code>zensical new</code> - Create a new project</li> <li><code>zensical serve</code> - Start local web server</li> <li><code>zensical build</code> - Build your site</li> </ul>","path":["技术相关"],"tags":[]},{"location":"techhelp/#examples","level":2,"title":"Examples","text":"","path":["技术相关"],"tags":[]},{"location":"techhelp/#admonitions","level":3,"title":"Admonitions","text":"<p>Go to documentation</p> <p>Note</p> <p>This is a note admonition. Use it to provide helpful information.</p> <p>Warning</p> <p>This is a warning admonition. Be careful!</p>","path":["技术相关"],"tags":[]},{"location":"techhelp/#details","level":3,"title":"Details","text":"<p>Go to documentation</p> Click to expand for more info <p>This content is hidden until you click to expand it. Great for FAQs or long explanations.</p>","path":["技术相关"],"tags":[]},{"location":"techhelp/#code-blocks","level":2,"title":"Code Blocks","text":"<p>Go to documentation</p> Code blocks<pre><code>def greet(name):\n    print(f\"Hello, {name}!\") # (1)!\n\ngreet(\"Python\")\n</code></pre> <ol> <li> <p>Go to documentation</p> <p>Code annotations allow to attach notes to lines of code.</p> </li> </ol> <p>Code can also be highlighted inline: <code>print(\"Hello, Python!\")</code>.</p>","path":["技术相关"],"tags":[]},{"location":"techhelp/#content-tabs","level":2,"title":"Content tabs","text":"<p>Go to documentation</p> PythonRust <pre><code>print(\"Hello from Python!\")\n</code></pre> <pre><code>println!(\"Hello from Rust!\");\n</code></pre>","path":["技术相关"],"tags":[]},{"location":"techhelp/#diagrams","level":2,"title":"Diagrams","text":"<p>Go to documentation</p> <pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];\n</code></pre>","path":["技术相关"],"tags":[]},{"location":"techhelp/#footnotes","level":2,"title":"Footnotes","text":"<p>Go to documentation</p> <p>Here's a sentence with a footnote.<sup>1</sup></p> <p>Hover it, to see a tooltip.</p>","path":["技术相关"],"tags":[]},{"location":"techhelp/#formatting","level":2,"title":"Formatting","text":"<p>Go to documentation</p> <ul> <li>This was marked (highlight)</li> <li>This was inserted (underline)</li> <li>This was deleted (strikethrough)</li> <li>H<sub>2</sub>O</li> <li>A<sup>T</sup>A</li> <li>Ctrl+Alt+Del</li> </ul>","path":["技术相关"],"tags":[]},{"location":"techhelp/#icons-emojis","level":2,"title":"Icons, Emojis","text":"<p>Go to documentation</p> <ul> <li> <code>:sparkles:</code></li> <li> <code>:rocket:</code></li> <li> <code>:tada:</code></li> <li> <code>:memo:</code></li> <li> <code>:eyes:</code></li> </ul>","path":["技术相关"],"tags":[]},{"location":"techhelp/#maths","level":2,"title":"Maths","text":"<p>Go to documentation</p> \\[ \\cos x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{(2k)!}x^{2k} \\] <p>Needs configuration</p> <p>Note that MathJax is included via a <code>script</code> tag on this page and is not configured in the generated default configuration to avoid including it in a pages that do not need it. See the documentation for details on how to configure it on all your pages if they are more Maths-heavy than these simple starter pages.</p>","path":["技术相关"],"tags":[]},{"location":"techhelp/#task-lists","level":2,"title":"Task Lists","text":"<p>Go to documentation</p> <ul> <li> Install Zensical</li> <li> Configure <code>zensical.toml</code></li> <li> Write amazing documentation</li> <li> Deploy anywhere</li> </ul>","path":["技术相关"],"tags":[]},{"location":"techhelp/#tooltips","level":2,"title":"Tooltips","text":"<p>Go to documentation</p> <p>Hover me</p> <ol> <li> <p>This is the footnote. ↩</p> </li> </ol>","path":["技术相关"],"tags":[]},{"location":"blog/class/ml_intro/","level":1,"title":"机器学习学习地图与哲学思考","text":"<p>一、s核心理解：从调色盘到机器学习</p> <p>一个直观的比喻：</p> <p>想象一个调色盘问题：我们想调出目标颜色，但只有几种基础原色可用。</p> <p>机器学习的过程类似于调色：</p> <ul> <li> <p>基础颜色 = 已有的参数</p> </li> <li> <p>调和过程 = 调整权重和偏差</p> </li> <li> <p>目标颜色 = 标签数据</p> </li> <li> <p>色差 = 损失函数（衡量预测与目标的差异）</p> </li> <li> <p>最优调色方向 = 梯度（使趋近速度最快的方向）</p> </li> </ul> <p>我们通过不断评估色差、寻找最佳调色方向、微调颜色配比，最终逼近目标色。虽然中间的具体步骤（哪些微量颜色的叠加产生了最终效果）难以完全被人理解（形成“黑箱”），但最终达到了目标。</p> <p>因此，机器学习的核心在于算法——这个自我优化（学习）的过程。而算法的选择又紧密依赖于具体的目标和场景。</p> <p>二、机器学习发展脉络</p> <p>时间线演进</p> <p>时期 年代 关键发展</p> <p>起始期 1980s-1990s BP神经网络、决策树、CNN雏形</p> <p>探索期 1990s-2010s SVM、Boosting、RNN、流形学习、随机森林、深度学习早期</p> <p>繁荣期 2010至今 深度学习时代：NLP、CV、ASR爆发，Transformer革命</p> <p>三、机器学习分类体系</p> <ol> <li>按输入数据类型分类</li> </ol> <p>• 结构化数据</p> <p>• 表格数据</p> <p>• 时间序列数据</p> <p>• 非结构化数据</p> <p>• 机器视觉（CV）算法</p> <p>• 自然语言处理（NLP）算法</p> <p>• 语音识别</p> <p>• 文本数据</p> <ol> <li>按模型架构分类</li> </ol> <p>• 传统算法</p> <p>• 线性回归、逻辑回归</p> <p>• 决策树、K近邻</p> <p>• 朴素贝叶斯、支持向量机</p> <p>• K均值聚类、PCA降维</p> <p>• 集成学习</p> <p>• Bagging（随机森林）</p> <p>• Boosting（AdaBoost、GBDT、XGBoost、LightGBM、CatBoost）</p> <p>• Stacking（基学习器+元学习器）</p> <p>• 神经网络（深度学习）</p> <p>• CNN、RNN</p> <p>• Transformer</p> <p>• 生成对抗网络（GAN）</p> <ol> <li>按学习范式分类</li> </ol> <p>• 监督学习</p> <p>• 分类（离散输出）</p> <p>• 回归（连续输出）</p> <p>• 无监督学习</p> <p>• 聚类</p> <p>• 降维（保留本质特征）</p> <p>• 强化学习</p> <p>• 策略优化</p> <p>• 控制与决策</p> <p>• 其他范式：半监督学习、自监督学习、迁移学习等</p> <ol> <li> <p>主要机器学习算法</p> </li> <li> <p>基于逻辑回归的分类预测</p> </li> <li>朴素贝叶斯</li> <li>K近邻</li> <li>支持向量机</li> <li>基于决策树的分类预测</li> <li>梯度提升树</li> <li>XGBoost</li> <li>LightGBM</li> <li>CatBoost</li> </ol> <p>四、深度学习专项</p> <ol> <li>数据工程</li> </ol> <p>• 数据收集/清洗：爬虫、数据标注、噪声处理</p> <p>• 特征工程：归一化、特征选择</p> <ol> <li>核心网络架构</li> </ol> <p>• 卷积神经网络：CNN，适用于图像处理、检测、分割、人脸识别</p> <p>• 循环神经网络：RNN，适用于NLP、时间序列预测、语音识别（现受Transformer冲击）</p> <p>• Transformer：自注意力机制，广泛应用于大语言模型，对先前架构产生革命性影响</p> <p>• 生成对抗网络：GAN</p> <ol> <li>训练与优化算法</li> </ol> <p>• 优化算法（如Adam优化器）</p> <p>• 正则化技术（如Dropout）</p> <p>五、哲学思考</p> <p>我坚信，当科学发展到前沿时，面临无方向的迷茫，决定未来的便是自身的哲学观：你如何理解世界？</p> <p>以我有限的见解，机器学习的本质仍是统计。那么，世间万物是否都可以被统计预测？宇宙的本质逻辑，是否就是一个超越人类心智结构的“黑箱”？</p> <p>附录：机器学习完整分类图谱</p> <p>graph TD     ML[机器学习分类] → Paradigm[按学习范式分类]     ML → Architecture[按模型架构分类]     ML → Task[按任务目标分类]     ML → Data[按数据特征分类]     ML → Training[按训练方式分类]</p> <pre><code>Paradigm --&gt; SL[监督学习]\nParadigm --&gt; UL[无监督学习]\nParadigm --&gt; RL[强化学习]\nParadigm --&gt; SSL[半监督学习]\nParadigm --&gt; SelfSL[自监督学习]\n\nArchitecture --&gt; Traditional[传统机器学习模型]\nArchitecture --&gt; NN[神经网络模型]\nArchitecture --&gt; Ensemble[集成学习方法]\n\nTraditional --&gt; Linear[线性模型]\nTraditional --&gt; Tree[基于树的模型]\nTraditional --&gt; SVM[支持向量机]\nTraditional --&gt; Bayes[贝叶斯模型]\n\nNN --&gt; FNN[前馈神经网络]\nNN --&gt; CNN[卷积神经网络]\nNN --&gt; RNN[循环神经网络]\nNN --&gt; Transformer[注意力与Transformer]\nNN --&gt; GAN[生成对抗网络]\nNN --&gt; GNN[图神经网络]\n\nTask --&gt; Prediction[预测任务]\nTask --&gt; Generation[生成任务]\nTask --&gt; Understanding[理解任务]\nTask --&gt; Decision[决策任务]\n\nData --&gt; Structured[结构化数据]\nData --&gt; Unstructured[非结构化数据]\nData --&gt; Graph[图结构数据]\n\nTraining --&gt; Batch[批量学习]\nTraining --&gt; Online[在线学习]\nTraining --&gt; Transfer[迁移学习]\nTraining --&gt; Meta[元学习]\nTraining --&gt; Federated[联邦学习]\n\n%% 详细子类（部分展开示例）\nSL --&gt; Classification[分类问题]\nSL --&gt; Regression[回归问题]\n\nClassification --&gt; Binary[二元分类]\nClassification --&gt; Multiclass[多元分类]\nClassification --&gt; Multilabel[多标签分类]\n\nCNN --&gt; ImageClass[图像分类网络&lt;br/&gt;ResNet, VGG]\nCNN --&gt; ObjectDetect[目标检测网络&lt;br/&gt;YOLO, Faster R-CNN]\nCNN --&gt; Segmentation[语义分割网络&lt;br/&gt;U-Net, DeepLab]\n\nTransformer --&gt; BERT[双向编码器]\nTransformer --&gt; GPT[自回归模型]\n\nstyle ML fill:#f9f,stroke:#333,stroke-width:2px\nstyle Paradigm fill:#bbf,stroke:#333\nstyle Architecture fill:#bfb,stroke:#333\nstyle Task fill:#fbb,stroke:#333\nstyle Data fill:#fbf,stroke:#333\nstyle Training fill:#ffb,stroke:#333\n</code></pre> <p>详细时间线</p> <p>• 1980s：传统统计方法主导</p> <p>• 1990s：SVM、集成方法兴起</p> <p>• 2000s：浅层神经网络、特征工程</p> <p>• 2012：深度学习突破（AlexNet）</p> <p>• 2014：GAN提出、注意力机制萌芽</p> <p>• 2017：Transformer架构革命</p> <p>• 2018：BERT/GPT预训练模型</p> <p>• 2020：大模型时代开启</p> <p>• 2023：多模态大模型爆发</p> <p>总结：机器学习是一个从直观调色比喻到复杂数学优化，从传统统计方法到深度学习革命，从具体算法到哲学思考的完整体系。理解其本质——通过算法从数据中学习模式并优化自身——是掌握这一领域的关键。s </p>","path":["课程","机器学习","机器学习学习地图与哲学思考"],"tags":[]},{"location":"blog/class/pb_intro/","level":1,"title":"Pb intro","text":"<p>随机变量及其分布是概率论的重点，本章主要讨论一维随机变量及其分布。</p>","path":["课程","概率论与统计学","Pb intro"],"tags":[]},{"location":"blog/class/pb_intro/#1","level":4,"title":"1. 随机变量及其分布函数","text":"<p>随机变量：定义在样本空间上的实值函数，常用大写字母表示，取值常用小写字母表示。</p> <p>分布函数：对于一个随机变量，定义\\(F(x)=P(X\\leq x)\\)为分布函数，称X服从\\(F(x)\\).满足下列三种基本性质： - 单调性 - 有界性 - 右连续性</p> <p>离散随机变量：分布列（非负性，正则性）</p> <p>连续随机变量：密度函数（非负性，正则性）</p>","path":["课程","概率论与统计学","Pb intro"],"tags":[]},{"location":"blog/class/pb_intro/#2","level":4,"title":"2. 随机变量的数学期望","text":"<p>根据分布计算随机变量的特征数（均值，方差，分位数），侧面反映分布的特征。下介绍最重要的特征数：数学期望。</p> <p>起源：分赌本问题 均值：算术平均，加权平均（根据频率） 离散表达式：\\(E(x)=\\sum x_{i}p(x_{i})\\)   (当且仅当绝对收敛的时候，此时唯一) 连续表达式：\\(E(s)=\\int xp(x) \\, dx\\) 物理解释：重心 统计学作用：消除随机性的重要手段</p> <p>数学期望的性质： \\(E(c)=c\\) \\(E(aX)=aE(x)\\) \\(E(g_{1}+g_{2})=E(g_{1})+E(g_{2})\\)</p>","path":["课程","概率论与统计学","Pb intro"],"tags":[]},{"location":"blog/class/pb_intro/#3","level":4,"title":"3. 随机变量的方差与标准差","text":"<p>方差： 对于随机变量\\(X\\)的均值a，定义新的随机变量\\((x-a)^2\\)，其均值称作方差。 \\(Var(X)=E(x-E(x))^2\\)</p> <p>标准差： 定义方差的平方根：\\(\\sqrt{ Var(x) }=\\sigma(X)\\)</p> <p>作用：都刻画\\(X\\)的波动程度（集中与分散程度）。方差/标准差越小，取值愈集中。</p> <p>注：由于标准差的量纲与数学期望一致，标准差的使用倾向更大。期望存在，方差不一定存在。但方差存在，期望一定存在。</p> <p>性质*： 1.\\(Var(X)=E(X^2)-[E(x)]^2\\) 2.常数的方差为0 3.\\(Var(aX+b)=a^2Var(X)\\) 4.切比雪夫不等式（chebyshev）： $$ P(\\mid X-E(x)|\\geq \\sigma) \\leq \\frac{Var(X)}{\\sigma^2} $$  对于切比雪夫不等式，描述了这样一个现实：  大偏差（\\(X-E(x)\\)）的概率，称作偏差发生概率，其上界与方差成正比，方差愈大，其上界愈大。</p>","path":["课程","概率论与统计学","Pb intro"],"tags":[]},{"location":"blog/class/pb_intro/#4","level":4,"title":"4. 常用的几种分布（离散）","text":"<p>二项分布   定义：描述在 \\( n \\) 次独立重复的伯努利试验中，成功次数 \\( X \\) 的概率分布。   概率质量函数$\\(P(X=k) = C_n^k p^k (1-p)^{n-k}, \\quad k=0,1,\\dots,n\\)$期望与方差：  $\\(E(X)=np,\\quad Var(X)=np(1-p)\\)$应用：适用于结果只有两种（成功/失败）的重复独立试验。</p> <p>泊松分布   定义：描述在单位时间（或空间）内，随机事件发生次数的概率分布。   概率质量函数：  $$   P(X=k)=\\frac{\\lambda^k e^{-\\lambda}}{k!}, \\quad k=0,1,2,\\dots$\\(**期望与方差**：\\)\\(E(X)=\\lambda,\\quad Var(X)=\\lambda\\)$应用：稀有事在一定时间或区间内发生次数的建模。</p> <p>关系：泊松分布可以作为二项分布的近似（泊松定理：当 \\(n\\) 很大、\\(p\\)  很小，且 \\(np\\) 适中时，泊松分布可作为二项分布的近似 \\(\\lambda = np\\) ）</p> <p>超几何分布  定义：描述在不放回抽样中，从包含 \\( M \\) 个成功个体和 \\( N-M \\) 个失败个体的总体中抽取 \\( n \\) 个个体，其中成功个体数 \\( X \\) 的分布。  概率质量函数：  $$   P(X=k)=\\frac{CM^k C<sup>{n-k}}{C_N</sup>n}$$期望与方差：  $$   E(X)=n\\cdot\\frac{M}{N},\\quad Var(X)=n\\cdot\\frac{M}{N}\\cdot\\left(1-\\frac{M}{N}\\right)\\cdot\\frac{N-n}{N-1}$$应用：适用于不放回抽样的质量控制或抽样调查。</p> <p>几何分布   定义：描述在多次独立伯努利试验中，首次成功所需试验次数 \\( X \\) 的概率分布。   概率质量函数：  $\\(P(X=k)=(1-p)^{k-1}p,\\quad k=1,2,\\dots\\)$期望与方差：  $$  E(X)=\\frac{1}{p},\\quad Var(X)=\\frac{1-p}{p^2}$$应用：用于建模在多次尝试中首次获得成功所需的次数。</p> <p>负二项分布   定义：描述在独立重复试验中，获得第 \\( r \\) 次成功所需试验次数 \\( X \\) 的概率分布。   概率质量函数：  $\\(P(X=k)=C_{k-1}^{r-1}p^r(1-p)^{k-r},\\quad k=r,r+1,\\dots\\)\\(**期望与方差**：\\)\\(E(X)=\\frac{r}{p},\\quad Var(X)=\\frac{r(1-p)}{p^2}\\)$应用：适用于需要多次成功的情境，如多次试验直到达成特定目标。</p>","path":["课程","概率论与统计学","Pb intro"],"tags":[]},{"location":"blog/class/pb_intro/#5","level":4,"title":"5. 常用的几种分布（连续）","text":"<p>正态分布   定义：又称高斯分布，是连续型概率分布中最重要的一种，由均值 \\(\\mu\\) 和方差 \\(\\sigma^2\\)决定。   概率密度函数：  $\\(f(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}},\\quad -\\infty&lt;x&lt;\\infty\\)$期望与方差：  $\\(E(X)=\\mu,\\quad Var(X)=\\sigma^2\\)$ 应用：广泛应用于自然科学和社会科学中的数据建模。</p> <p>均匀分布   定义：在区间 \\([a,b]\\) 内，每个点出现的概率相等的分布。   概率密度函数：  $$f(x)=\\begin{cases}   \\frac{1}{b-a}, &amp; a\\le x\\le b \\   0, &amp; \\text{otherwise}   \\end{cases} $$期望与方差：  $$   E(X)=\\frac{a+b}{2},\\quad Var(X)=\\frac{(b-a)^2}{12}$$应用：适用于在某个区间内等可能取值的随机现象。</p> <p>指数分布   定义：描述泊松过程中事件发生时间间隔的概率分布。   概率密度函数：  $\\(f(x)=\\lambda e^{-\\lambda x},\\quad x\\ge0\\)$期望与方差：  $\\(E(X)=\\frac{1}{\\lambda},\\quad Var(X)=\\frac{1}{\\lambda^2}\\)$应用：常用于可靠性分析和排队论，描述无记忆性的随机时间间隔。</p> <p>伽马分布   定义：指数分布的推广，描述多个独立指数分布随机变量之和的分布。   概率密度函数：  $$ f(x)=\\frac{\\beta<sup>\\alpha}{\\Gamma(\\alpha)}x</sup>,\\quad x&gt;0$$}e^{-\\beta x期望与方差：  $$   E(X)=\\frac{\\alpha}{\\beta},\\quad Var(X)=\\frac{\\alpha}{\\beta^2} $$应用：适用于对连续正随机变量建模，如等待时间的总和。</p> <p>贝塔分布   定义：定义在区间 \\([0,1]\\) 上的连续概率分布，由两个正参数 \\(\\alpha\\) 和 \\(\\beta\\)  决定。   概率密度函数：  $\\(f(x)=\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha,\\beta)},\\quad 0\\le x\\le1\\)$期望与方差：  $\\(E(X)=\\frac{\\alpha}{\\alpha+\\beta},\\quad Var(X)=\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\)$应用：常用于表示概率的概率分布，适用于比例或百分比的建模。</p> <p>卡方分布   定义：是 k 个独立标准正态随机变量的平方和所服从的分布，其中k为自由度。   概率密度函数：  $\\(f(x)=\\frac{1}{2^{k/2}\\Gamma(k/2)}x^{k/2-1}e^{-x/2},\\quad x&gt;0\\)$期望与方差：  $$ E(X)=k,\\quad Var(X)=2k$$应用：主要用于假设检验和置信区间的构建，特别是方差分析和拟合优度检验。</p>","path":["课程","概率论与统计学","Pb intro"],"tags":[]},{"location":"blog/class/pb_intro/#6","level":4,"title":"6. 分布的其他特征数","text":"<p>\\(k\\)阶原点矩 - \\(\\mu_{k}= E(X^k)\\) - 一阶原点矩：数学期望</p> <p>\\(k\\)阶中心矩 - \\(\\nu_{k}= E(X-E(X))^k\\) - 二阶中心矩：方差</p> <p>二者存在对应关系</p> <p>变异系数：标准差比期望（消除量纲的影响）</p> <p>分位数：分为两块（下侧，上侧）</p> <p>中位数：均分的分位数</p> <p>偏度系数：描述分布偏离对称性的程度（三阶中心矩/二阶中心矩的3/2次方）</p> <p>峰度系数：描述分布尖峭程度或尾部粗细（四阶中心矩/二阶中心矩的2次方 -3）</p>","path":["课程","概率论与统计学","Pb intro"],"tags":[]},{"location":"blog/class/pb_question/","level":1,"title":"Pb question","text":"<p>Question</p> <p>概率论和统计学的区别是什么？</p> <p>概率论是对已有的数据分析，得到其特征和规律。 统计学是根据已有的结果，推断背后的规律。</p> <p>Question</p> <p>能否给出一个能串联的脉络？</p> <p>概率论：公理化（定义概率）——大数定理（均值收敛/期望）——中心极限定理（分布收敛/归一化）</p> <p>统计学：现实数据（描述）——抽样分布理论（特征）——参数估计（估计）——假设检验（决策）</p> <p>Question</p> <p>说明一下大数定理？</p> <p>大数定理指出：样本均值趋于总体均值（真实期望） 关心的是样本均值的收敛性（这个点依概率收敛到总体期望） 提供了稳定性。</p> <p>Question</p> <p>说明一下中心极限定理？</p> <p>中心极限定理：独立同分布的随机变量趋于正态。 关心的是样本均值的分布形状（标准化后收敛到正态分布） 为估计提供了量化工具来计算概率。</p> <p>Question</p> <p>如何进行参数估计，或者说方法？</p> <p>对于点估计，主要最大似然估计、矩估计。 对于区间估计，抽样分布理论 t分布，f分布，正态分布，卡方分布</p> <p>Question</p> <p>如何判断一个估计量的好坏？</p> <p>无偏性，有效性，一致性。</p> <p>Question</p> <p>有哪些补充与扩展？</p> <p>比如公理化定义后，后续证明的基石，导向了均值（大数定理）和分布（中心极限定理）的马尔可夫不等式，切比雪夫不等式。</p> <p>比如估计的方法上，主要包括最大似然估计，矩估计。</p> <p>Question</p> <p>对于概率的理解存在的哲学之争？</p> <p>长期以来，频率学派一直占据重要的主导地位，但随着计算机的兴起，贝叶斯学派接受的人越来越多。 贝叶斯学派：概率是对不确定性的主观信任程度，认为参数是随机的。 传统频率学派：概率是大量实验中的比例（频率）。</p> <p>Question</p> <p>概率论在现代领域的应用？</p> <p>在数据科学和机器学习上，概率统计作为理论的基石，不论思想还是方法上都是处理的重要工具。</p> <p>Question</p> <p>后续的发展</p> <p>（1）随着已有的基础理论不断完善，对现实世界动态波动的概率的研究成为主流，建立了随机过程这一基础上的领域。</p> <p>马尔可夫过程（未来只依赖现在）——鞅论——随机分析 泊松过程 布朗运动</p> <p>（3）面向认知与信息</p> <p>信息论：熵，互信息，KL散度。</p>","path":["课程","概率论与统计学","Pb question"],"tags":[]},{"location":"blog/essay/huiyilu/","level":1,"title":"Huiyilu","text":"<p>啊啊</p>","path":["随笔","Huiyilu"],"tags":[]},{"location":"blog/posts/2025-01-22-hello-world/","level":1,"title":"我的第一篇博客","text":"<p>这是文章内容...</p>","path":["我的第一篇博客"],"tags":["Zensical","教程"]},{"location":"blog/posts/links_ai/","level":1,"title":"AI网站导航","text":"","path":["档案","AI网站导航"],"tags":[]},{"location":"blog/posts/links_ai/#_1","level":2,"title":"综合平台","text":"<ul> <li>@Kaggle - 数据科学竞赛与社区平台</li> <li>@Hugging Face - 开源模型与数据集平台</li> <li>@Papers with Code - 论文与代码集合</li> </ul>","path":["档案","AI网站导航"],"tags":[]},{"location":"blog/posts/links_ai/#ai_1","level":2,"title":"AI模型与工具","text":"<ul> <li>@OpenAI - ChatGPT、GPT系列模型</li> <li>@Anthropic Claude - Claude智能助手</li> <li>@Google Gemini - Google AI助手</li> <li>@Midjourney - AI图像生成</li> <li>@Stable Diffusion - 开源图像生成模型</li> <li>@DeepSeek - 深度求索AI助手</li> </ul>","path":["档案","AI网站导航"],"tags":[]},{"location":"blog/posts/links_ai/#_2","level":2,"title":"开发与代码","text":"<ul> <li>@GitHub Copilot - AI编程助手</li> <li>@Replit - 在线AI编程环境</li> <li>@Cursor - AI代码编辑器</li> </ul>","path":["档案","AI网站导航"],"tags":[]},{"location":"blog/posts/links_ai/#_3","level":2,"title":"搜索与学习","text":"<ul> <li>@Perplexity - AI搜索引擎</li> <li>@arXiv - 学术论文预印本</li> <li>@Google AI - Google AI研究</li> </ul>","path":["档案","AI网站导航"],"tags":[]},{"location":"blog/posts/links_book/","level":1,"title":"书籍汇总","text":"<p>书籍</p>","path":["书籍汇总"],"tags":[]},{"location":"blog/posts/links_math/","level":1,"title":"一些数学的网站","text":"<ul> <li>@刘思齐老师的主页 - 包含录课视频 </li> <li>@Math Stack Exchange - 数学问答社区</li> <li>@李文威老师的主页 - 代数学方法</li> <li>@谢启鸿老师的博客 - 高等代数方面</li> <li>@香蕉空间 - 类wiki的知识库</li> <li>@Overleaf - 在线LaTeX编辑平台</li> <li>@Beamer主题矩阵 - Beamer主题预览与配色</li> </ul>","path":["档案","一些数学的网站"],"tags":[]},{"location":"blog/posts/seminar/","level":1,"title":"Seminar & Project","text":"","path":["Seminar & Project"],"tags":[]},{"location":"blog/posts/seminar/#_1","level":2,"title":"研讨会","text":"<p>待续……</p>","path":["Seminar & Project"],"tags":[]},{"location":"blog/posts/seminar/#_2","level":2,"title":"项目","text":"<p>数学和生物学类：</p> <ul> <li>一类生态学复杂动力系统涌现现象的研究</li> </ul> <p>代数学和密码学类：</p> <ul> <li>基于有理变换的一些不可约多项式的构造</li> </ul> <p>机器学习类：</p> <ul> <li> <p>肝脏与胰腺医学影像中的人工智能应用</p> </li> <li> <p>基于SIFT算法的特征点图像拼接</p> </li> </ul>","path":["Seminar & Project"],"tags":[]}]}