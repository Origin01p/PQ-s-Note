{"config":{"separator":"[\\s\\u200b\\-]"},"items":[{"location":"","level":1,"title":"PQ World","text":"Hello,World! |          Github                 Email me        星火世传 奋飞不辍 加载中... <ul> <li> <p> 推荐的文章</p> </li> <li> <p> 一些课程</p> <ul> <li>概率论与统计</li> <li>实变函数与泛函分析</li> <li>复变函数</li> </ul> <ul> <li>机器学习(新) →</li> </ul> </li> <li> <p> 整理/汇总</p> <ul> <li>AI网站</li> <li>一些数学家的博客</li> <li>书目汇总</li> <li>参与过的项目或研讨会 </li> </ul> </li> <li> <p> 关于</p> <ul> <li>留言板<sup>1</sup></li> <li>博客</li> <li> 关于我</li> </ul> </li> </ul> <ol> <li> <p>梦醒人间看微雨，江山还似旧温柔 ↩</p> </li> </ol>","path":["PQ World"],"tags":[]},{"location":"about/","level":1,"title":"关于我","text":"<p>本网站旨在个人部分学习资料的保存与展示。</p>","path":["关于"],"tags":[]},{"location":"about/#education","level":3,"title":"Education","text":"<ul> <li>2021.09 - 2025.06 中南大学，数学与统计学院</li> </ul>","path":["关于"],"tags":[]},{"location":"about/#experience","level":3,"title":"Experience","text":"<ul> <li>2026.01 — 至今 麻省理工学院（MIT）微硕士项目：统计学与数据科学（MicroMasters® Program in Statistics and Data Science）</li> </ul>","path":["关于"],"tags":[]},{"location":"about/#others","level":3,"title":"Others","text":"<ul> <li>Email: origin01p@outlook.com / pengwq7@gmail.com(备)</li> <li>@Github主页</li> <li>@Kaggle主页</li> </ul>","path":["关于"],"tags":[]},{"location":"about/#thanks","level":2,"title":"Thanks","text":"<p>感谢@Wcowin的技术支持！</p>","path":["关于"],"tags":[]},{"location":"archive/","level":1,"title":"归档","text":"<p>历年笔记</p>","path":["档案","归档"],"tags":[]},{"location":"techhelp/","level":1,"title":"个人","text":"<p>For full documentation visit zensical.org.</p>","path":["技术相关"],"tags":[]},{"location":"techhelp/#commands","level":2,"title":"Commands","text":"<ul> <li><code>zensical new</code> - Create a new project</li> <li><code>zensical serve</code> - Start local web server</li> <li><code>zensical build</code> - Build your site</li> </ul>","path":["技术相关"],"tags":[]},{"location":"techhelp/#examples","level":2,"title":"Examples","text":"","path":["技术相关"],"tags":[]},{"location":"techhelp/#admonitions","level":3,"title":"Admonitions","text":"<p>Go to documentation</p> <p>Note</p> <p>This is a note admonition. Use it to provide helpful information.</p> <p>Warning</p> <p>This is a warning admonition. Be careful!</p>","path":["技术相关"],"tags":[]},{"location":"techhelp/#details","level":3,"title":"Details","text":"<p>Go to documentation</p> Click to expand for more info <p>This content is hidden until you click to expand it. Great for FAQs or long explanations.</p>","path":["技术相关"],"tags":[]},{"location":"techhelp/#code-blocks","level":2,"title":"Code Blocks","text":"<p>Go to documentation</p> Code blocks<pre><code>def greet(name):\n    print(f\"Hello, {name}!\") # (1)!\n\ngreet(\"Python\")\n</code></pre> <ol> <li> <p>Go to documentation</p> <p>Code annotations allow to attach notes to lines of code.</p> </li> </ol> <p>Code can also be highlighted inline: <code>print(\"Hello, Python!\")</code>.</p>","path":["技术相关"],"tags":[]},{"location":"techhelp/#content-tabs","level":2,"title":"Content tabs","text":"<p>Go to documentation</p> PythonRust <pre><code>print(\"Hello from Python!\")\n</code></pre> <pre><code>println!(\"Hello from Rust!\");\n</code></pre>","path":["技术相关"],"tags":[]},{"location":"techhelp/#diagrams","level":2,"title":"Diagrams","text":"<p>Go to documentation</p> <pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];\n</code></pre>","path":["技术相关"],"tags":[]},{"location":"techhelp/#footnotes","level":2,"title":"Footnotes","text":"<p>Go to documentation</p> <p>Here's a sentence with a footnote.<sup>1</sup></p> <p>Hover it, to see a tooltip.</p>","path":["技术相关"],"tags":[]},{"location":"techhelp/#formatting","level":2,"title":"Formatting","text":"<p>Go to documentation</p> <ul> <li>This was marked (highlight)</li> <li>This was inserted (underline)</li> <li>This was deleted (strikethrough)</li> <li>H<sub>2</sub>O</li> <li>A<sup>T</sup>A</li> <li>Ctrl+Alt+Del</li> </ul>","path":["技术相关"],"tags":[]},{"location":"techhelp/#icons-emojis","level":2,"title":"Icons, Emojis","text":"<p>Go to documentation</p> <ul> <li> <code>:sparkles:</code></li> <li> <code>:rocket:</code></li> <li> <code>:tada:</code></li> <li> <code>:memo:</code></li> <li> <code>:eyes:</code></li> </ul>","path":["技术相关"],"tags":[]},{"location":"techhelp/#maths","level":2,"title":"Maths","text":"<p>Go to documentation</p> \\[ \\cos x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{(2k)!}x^{2k} \\] <p>Needs configuration</p> <p>Note that MathJax is included via a <code>script</code> tag on this page and is not configured in the generated default configuration to avoid including it in a pages that do not need it. See the documentation for details on how to configure it on all your pages if they are more Maths-heavy than these simple starter pages.</p>","path":["技术相关"],"tags":[]},{"location":"techhelp/#task-lists","level":2,"title":"Task Lists","text":"<p>Go to documentation</p> <ul> <li> Install Zensical</li> <li> Configure <code>zensical.toml</code></li> <li> Write amazing documentation</li> <li> Deploy anywhere</li> </ul>","path":["技术相关"],"tags":[]},{"location":"techhelp/#tooltips","level":2,"title":"Tooltips","text":"<p>Go to documentation</p> <p>Hover me</p> <ol> <li> <p>This is the footnote. ↩</p> </li> </ol>","path":["技术相关"],"tags":[]},{"location":"blog/class/ml_gbdt/","level":1,"title":"GBDT","text":"<ul> <li>归属：集成学习中的Boosting。（进阶也是常用：XGboost）</li> <li>过程：利用多个决策树（通常是CART树）逐步拟合数据，新加入的树在前一棵树的残差基础上学习。</li> <li>核心思想：梯度下降。将损失函数的负梯度作为残差的近似，从而序列化构建每一颗决策树，将所有树的预测结果加权得到集成模型。</li> <li>优点：可解释性强、预测精度高</li> <li>缺点：计算复杂度高、串行生成故并行难</li> </ul>","path":["课程","机器学习","梯度提升决策树"],"tags":[]},{"location":"blog/class/ml_gbdt/#_1","level":2,"title":"重要参数","text":"<ul> <li>n_estimators：树的数量（迭代次数），太少会欠拟合，太多会过拟合且计算慢。通常配合早停法使用。</li> <li>learning_rate：学习率（每棵树的贡献权重）。</li> <li>max_depth：树的最大深度。</li> <li>maxleafnodes：最大叶节点数。</li> </ul>","path":["课程","机器学习","梯度提升决策树"],"tags":[]},{"location":"blog/class/ml_intro/","level":1,"title":"机器学习学习地图与哲学思考","text":"<p>一、s核心理解：从调色盘到机器学习</p> <p>一个直观的比喻：</p> <p>想象一个调色盘问题：我们想调出目标颜色，但只有几种基础原色可用。</p> <p>机器学习的过程类似于调色：</p> <ul> <li> <p>基础颜色 = 已有的参数</p> </li> <li> <p>调和过程 = 调整权重和偏差</p> </li> <li> <p>目标颜色 = 标签数据</p> </li> <li> <p>色差 = 损失函数（衡量预测与目标的差异）</p> </li> <li> <p>最优调色方向 = 梯度（使趋近速度最快的方向）</p> </li> </ul> <p>我们通过不断评估色差、寻找最佳调色方向、微调颜色配比，最终逼近目标色。虽然中间的具体步骤（哪些微量颜色的叠加产生了最终效果）难以完全被人理解（形成“黑箱”），但最终达到了目标。</p> <p>因此，机器学习的核心在于算法——这个自我优化（学习）的过程。而算法的选择又紧密依赖于具体的目标和场景。</p> <p>二、机器学习发展脉络</p> <p>时间线演进</p> <p>时期 年代 关键发展</p> <p>起始期 1980s-1990s BP神经网络、决策树、CNN雏形</p> <p>探索期 1990s-2010s SVM、Boosting、RNN、流形学习、随机森林、深度学习早期</p> <p>繁荣期 2010至今 深度学习时代：NLP、CV、ASR爆发，Transformer革命</p> <p>三、机器学习分类体系</p> <ol> <li>按输入数据类型分类</li> </ol> <p>• 结构化数据</p> <p>• 表格数据</p> <p>• 时间序列数据</p> <p>• 非结构化数据</p> <p>• 机器视觉（CV）算法</p> <p>• 自然语言处理（NLP）算法</p> <p>• 语音识别</p> <p>• 文本数据</p> <ol> <li>按模型架构分类</li> </ol> <p>• 传统算法</p> <p>• 线性回归、逻辑回归</p> <p>• 决策树、K近邻</p> <p>• 朴素贝叶斯、支持向量机</p> <p>• K均值聚类、PCA降维</p> <p>• 集成学习</p> <p>• Bagging（随机森林）</p> <p>• Boosting（AdaBoost、GBDT、XGBoost、LightGBM、CatBoost）</p> <p>• Stacking（基学习器+元学习器）</p> <p>• 神经网络（深度学习）</p> <p>• CNN、RNN</p> <p>• Transformer</p> <p>• 生成对抗网络（GAN）</p> <ol> <li>按学习范式分类</li> </ol> <p>• 监督学习</p> <p>• 分类（离散输出）</p> <p>• 回归（连续输出）</p> <p>• 无监督学习</p> <p>• 聚类</p> <p>• 降维（保留本质特征）</p> <p>• 强化学习</p> <p>• 策略优化</p> <p>• 控制与决策</p> <p>• 其他范式：半监督学习、自监督学习、迁移学习等</p> <ol> <li> <p>主要机器学习算法</p> </li> <li> <p>基于逻辑回归的分类预测</p> </li> <li>朴素贝叶斯</li> <li>K近邻</li> <li>支持向量机</li> <li>基于决策树的分类预测</li> <li>梯度提升树</li> <li>XGBoost</li> <li>LightGBM</li> <li>CatBoost</li> </ol> <p>四、深度学习专项</p> <ol> <li>数据工程</li> </ol> <p>• 数据收集/清洗：爬虫、数据标注、噪声处理</p> <p>• 特征工程：归一化、特征选择</p> <ol> <li>核心网络架构</li> </ol> <p>• 卷积神经网络：CNN，适用于图像处理、检测、分割、人脸识别</p> <p>• 循环神经网络：RNN，适用于NLP、时间序列预测、语音识别（现受Transformer冲击）</p> <p>• Transformer：自注意力机制，广泛应用于大语言模型，对先前架构产生革命性影响</p> <p>• 生成对抗网络：GAN</p> <ol> <li>训练与优化算法</li> </ol> <p>• 优化算法（如Adam优化器）</p> <p>• 正则化技术（如Dropout）</p> <p>五、哲学思考</p> <p>我坚信，当科学发展到前沿时，面临无方向的迷茫，决定未来的便是自身的哲学观：你如何理解世界？</p> <p>以我有限的见解，机器学习的本质仍是统计。那么，世间万物是否都可以被统计预测？宇宙的本质逻辑，是否就是一个超越人类心智结构的“黑箱”？</p> <p>附录：机器学习完整分类图谱</p> <p>graph TD     ML[机器学习分类] → Paradigm[按学习范式分类]     ML → Architecture[按模型架构分类]     ML → Task[按任务目标分类]     ML → Data[按数据特征分类]     ML → Training[按训练方式分类]</p> <pre><code>Paradigm --&gt; SL[监督学习]\nParadigm --&gt; UL[无监督学习]\nParadigm --&gt; RL[强化学习]\nParadigm --&gt; SSL[半监督学习]\nParadigm --&gt; SelfSL[自监督学习]\n\nArchitecture --&gt; Traditional[传统机器学习模型]\nArchitecture --&gt; NN[神经网络模型]\nArchitecture --&gt; Ensemble[集成学习方法]\n\nTraditional --&gt; Linear[线性模型]\nTraditional --&gt; Tree[基于树的模型]\nTraditional --&gt; SVM[支持向量机]\nTraditional --&gt; Bayes[贝叶斯模型]\n\nNN --&gt; FNN[前馈神经网络]\nNN --&gt; CNN[卷积神经网络]\nNN --&gt; RNN[循环神经网络]\nNN --&gt; Transformer[注意力与Transformer]\nNN --&gt; GAN[生成对抗网络]\nNN --&gt; GNN[图神经网络]\n\nTask --&gt; Prediction[预测任务]\nTask --&gt; Generation[生成任务]\nTask --&gt; Understanding[理解任务]\nTask --&gt; Decision[决策任务]\n\nData --&gt; Structured[结构化数据]\nData --&gt; Unstructured[非结构化数据]\nData --&gt; Graph[图结构数据]\n\nTraining --&gt; Batch[批量学习]\nTraining --&gt; Online[在线学习]\nTraining --&gt; Transfer[迁移学习]\nTraining --&gt; Meta[元学习]\nTraining --&gt; Federated[联邦学习]\n\n%% 详细子类（部分展开示例）\nSL --&gt; Classification[分类问题]\nSL --&gt; Regression[回归问题]\n\nClassification --&gt; Binary[二元分类]\nClassification --&gt; Multiclass[多元分类]\nClassification --&gt; Multilabel[多标签分类]\n\nCNN --&gt; ImageClass[图像分类网络&lt;br/&gt;ResNet, VGG]\nCNN --&gt; ObjectDetect[目标检测网络&lt;br/&gt;YOLO, Faster R-CNN]\nCNN --&gt; Segmentation[语义分割网络&lt;br/&gt;U-Net, DeepLab]\n\nTransformer --&gt; BERT[双向编码器]\nTransformer --&gt; GPT[自回归模型]\n\nstyle ML fill:#f9f,stroke:#333,stroke-width:2px\nstyle Paradigm fill:#bbf,stroke:#333\nstyle Architecture fill:#bfb,stroke:#333\nstyle Task fill:#fbb,stroke:#333\nstyle Data fill:#fbf,stroke:#333\nstyle Training fill:#ffb,stroke:#333\n</code></pre> <p>详细时间线</p> <p>• 1980s：传统统计方法主导</p> <p>• 1990s：SVM、集成方法兴起</p> <p>• 2000s：浅层神经网络、特征工程</p> <p>• 2012：深度学习突破（AlexNet）</p> <p>• 2014：GAN提出、注意力机制萌芽</p> <p>• 2017：Transformer架构革命</p> <p>• 2018：BERT/GPT预训练模型</p> <p>• 2020：大模型时代开启</p> <p>• 2023：多模态大模型爆发</p> <p>总结：机器学习是一个从直观调色比喻到复杂数学优化，从传统统计方法到深度学习革命，从具体算法到哲学思考的完整体系。理解其本质——通过算法从数据中学习模式并优化自身——是掌握这一领域的关键。s </p>","path":["课程","机器学习","机器学习导论"],"tags":[]},{"location":"blog/class/ml_rf/","level":1,"title":"Random Forest","text":"<ul> <li>核心思想：群体智慧</li> <li>归属： 集成学习中的Bagging（类并行）</li> <li>优点： 准确率高，抗过拟合强，对数据要求友好</li> <li>缺点： 计算成本高，可解释性差，预测速度慢</li> </ul>","path":["课程","机器学习","随机森林"],"tags":[]},{"location":"blog/class/ml_rf/#_1","level":2,"title":"过程","text":"<ul> <li>数据采集：有放回的随机抽样（bootstrap采样），生成多个子数据集</li> <li>构建决策树：每次分裂只考虑部分随即特征（通常是总特征数的开方）</li> <li>训练决策树</li> <li>聚合预测：分类投票，回归平均</li> <li>输出</li> </ul>","path":["课程","机器学习","随机森林"],"tags":[]},{"location":"blog/class/ml_rf/#_2","level":2,"title":"重要参数","text":"<ul> <li>n_estimators：树的数量（计算量）</li> <li>max_depth：最大树深（控制复杂度）</li> <li>minsamplessplit：分裂内部节点所需最小样本数</li> <li>minsamplesleaf：叶节点最少样本数</li> <li>max_features：寻找最佳分裂时考虑的特征数</li> <li>bootstrap：是否使用bootstrap抽样</li> <li>criterion：分裂标准（gini/entropy）</li> <li>random_state：随机种子</li> <li>n_jobs：并行运行的核心数</li> <li>oob_score：是否使用袋外样本评估</li> </ul>","path":["课程","机器学习","随机森林"],"tags":[]},{"location":"blog/class/ml_rf/#_3","level":2,"title":"代码","text":"<pre><code>  import numpy as np\n  import pandas as pd\n  from sklearn.ensemble import RandomForestClassifier #分类\n  from sklearn.ensemble import RandomForestRegressor  #回归\n</code></pre>","path":["课程","机器学习","随机森林"],"tags":[]},{"location":"blog/class/ml_ridge/","level":1,"title":"岭回归","text":"<p>Ridge</p> <ul> <li>矩阵对角线添加常数项改善稳定性</li> <li>特征数量很多甚至超过样本数量</li> <li>当特征变量间存在高度相关性</li> </ul> <p>普通线性回归损失函数</p> \\[ J(β) = Σ(y_i - ŷ_i)² \\] <p>岭回归损失函数：</p> \\[ J(β) = Σ(y_i - ŷ_i)² + λ × Σβ_j² \\] <p>比较可以看出岭回归加入了L2正则化项，对模型系数大小进行约束，防止过大，有效防止过拟合。（L1项则为Lasso回归） 其中\\(\\lambda\\)作为正则化强度参数，代表惩罚力度，越大模型越简单（压缩），等于0时，退化为普通线性回归。</p> <p>重新引申两个重要概念，偏差与方差：</p> <p>偏差：</p> <ul> <li>模型预测值和真实值间的系统误差。</li> <li>模型过于简单，容易导致高偏差（欠拟合）</li> </ul> <p>方差：</p> <ul> <li>模型对训练数据波动的敏感程度。</li> <li>模型过于复杂，容易对噪声过度拟合（过拟合）</li> </ul>","path":["课程","机器学习","岭回归"],"tags":[]},{"location":"blog/class/ml_xgboost/","level":1,"title":"XGboost","text":"","path":["课程","机器学习","XGboost"],"tags":[]},{"location":"blog/class/ml_xgboost/#_1","level":2,"title":"简介","text":"<p>XGboost（eXtreme Gradient Boosting）直译极度梯度提升。</p> <ul> <li>归属：集成学习—Boosting—梯度提升决策树（GBDT）—XGBoost</li> <li>优点：简单高效可扩展，可自动处理缺失值。</li> <li>缺点：在大数据处理环节，精度不如深度学习</li> </ul>","path":["课程","机器学习","XGboost"],"tags":[]},{"location":"blog/class/ml_xgboost/#gbdt","level":2,"title":"进步和提升（相比GBDT）","text":"<ul> <li>采用二阶泰勒展开：优化损失函数，使得梯度下降更精准，收敛更快。</li> <li>正则化：有效防止过拟合</li> <li>支持并行计算：特征选择，Blocks存储。</li> </ul>","path":["课程","机器学习","XGboost"],"tags":[]},{"location":"blog/class/ml_xgboost/#_2","level":2,"title":"重要参数","text":"<ul> <li>eta：学习率，默认0.3</li> <li>booster：基学习器类型，常见有\"gbtree\"（树模型），\"gblinear\"（线性模型）,\"dart\"（带Dropout树）</li> <li>minchildweight： 叶子节点样本权重和的最小值</li> <li>max_depth：树的最大深度</li> <li>n_estimators：树的数量</li> <li>subsample：每棵树使用的样本比例（行采样）</li> <li>colsample_bytree：每棵树使用的特征比例（列采样）</li> <li>gamma：最小分裂增益</li> <li>alpha/lambda：L1，L2正则化系数</li> <li>objective：定义任务类型（如二分类、多分类、回归）</li> </ul>","path":["课程","机器学习","XGboost"],"tags":[]},{"location":"blog/class/ml_xgboost/#_3","level":2,"title":"数学表达式","text":"<p>目标函数：</p> \\[ \\text{Obj}(\\theta) = \\sum_{i=1}^{n} L(y_i, \\hat{y}_i) + \\sum_{k=1}^{K} \\Omega(f_k) \\] <p>前者为损失项（预测值与真实值的差），后者为正则化项（L2，L1，衡量模型复杂度）： $$ \\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda |w|^2 + \\alpha |w|_1 $$</p> <p>泰勒展开（二阶）近似：</p> \\[ \\begin{aligned} \\text{Obj}^{(t)} &amp;= \\sum_{i=1}^{n} L(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) + \\Omega(f_t) \\\\ &amp;\\approx \\sum_{i=1}^{n} \\left[ L(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i) \\right] + \\Omega(f_t) \\end{aligned} \\]","path":["课程","机器学习","XGboost"],"tags":[]},{"location":"blog/class/ml_xgboost/#_4","level":2,"title":"代码","text":"<pre><code> from xgboost import XGBClassifier\n from sklearn.model_selection import train_test_split\n\n model = XGBClassifier(\n     n_estimators = 100, \n     max_depth = 3,\n     learning_rate = 0.1\n)\n\n model.fit(X_train, Y_train)\n y_pred = model.predict(X_test)\n</code></pre>","path":["课程","机器学习","XGboost"],"tags":[]},{"location":"blog/class/pb_ch1/","level":1,"title":"第1章 随机事件与概率","text":"","path":["课程","概率论与统计学","第1章 随机事件与概率"],"tags":[]},{"location":"blog/class/pb_ch1/#1","level":2,"title":"1.意义","text":"<ul> <li>研究对象：随机现象</li> <li>概率论研究：随机现象的模型（概率分布）</li> <li>数理统计研究：随机现象的数据收集，处理，统计推断</li> </ul>","path":["课程","概率论与统计学","第1章 随机事件与概率"],"tags":[]},{"location":"blog/class/pb_ch1/#2","level":2,"title":"2.术语","text":"<ul> <li>样本空间：一切可能的结果的集合，记为 \\(\\Omega\\) 或 \\(S\\)。其中元素 \\(\\omega\\) 称作样本点。分为离散（有限或可列个）及连续（不可列无限）。</li> <li>随机事件 :  样本空间中某些样本点组成的子集，简称事件。包含：基本事件（单元素集），必然事件（最大子集/本身），不可能事件（空集 \\(\\varnothing\\)）。</li> <li>随机变量 ：表示随机现象结果的变量，常用大写字母 \\(X, Y, Z, \\ldots\\) 表示。</li> </ul>","path":["课程","概率论与统计学","第1章 随机事件与概率"],"tags":[]},{"location":"blog/class/pb_ch1/#3","level":2,"title":"3.关系及运算","text":"<p>事件间的关系：</p> <ul> <li>包含（\\(A \\subset B\\)）</li> <li>相等（\\(A = B\\)）</li> <li>互不相容（\\(A \\cap B = \\varnothing\\)）</li> </ul> <p>运算：</p> <ul> <li>并（和）：\\(A\\bigcup B\\)</li> <li>差：\\(A-B\\) 或 \\(A \\backslash B\\)</li> <li>对立（补）：\\(\\bar{A}\\) 或 \\(A^c\\)</li> </ul> <p>运算规则：</p> <ul> <li>交换律：\\(A \\cup B = B \\cup A,\\quad A \\cap B = B \\cap A\\)</li> <li>结合律：\\((A \\cup B) \\cup C = A \\cup (B \\cup C),\\quad (A \\cap B) \\cap C = A \\cap (B \\cap C)\\)</li> <li>分配律：\\(A \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C),\\quad A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C)\\)</li> <li>对偶律（德摩根公式）：\\(\\overline{A \\cup B} = \\bar{A} \\cap \\bar{B},\\quad \\overline{A \\cap B} = \\bar{A} \\cup \\bar{B}\\)</li> </ul>","path":["课程","概率论与统计学","第1章 随机事件与概率"],"tags":[]},{"location":"blog/class/pb_ch1/#4","level":2,"title":"4.引申","text":"<p>事件域（Event Field / σ-algebra）：\\(\\Omega\\) 的子集类 \\(\\mathcal{F}\\)，满足：</p> <ol> <li>\\(\\Omega \\in \\mathcal{F}\\)；</li> <li>若 \\(A \\in \\mathcal{F}\\)，则 \\(\\bar{A} \\in \\mathcal{F}\\)；</li> <li>若 \\(A_n \\in \\mathcal{F} (n=1,2,\\ldots)\\)，则 \\(\\bigcup_{n=1}^{\\infty} A_n \\in \\mathcal{F}\\)。</li> </ol> <ul> <li>\\(\\mathcal{F}\\) 是由基本子集经过可列次并、交、补运算生成的集合类。</li> <li>常见的：Borel域（Borel σ-algebra），由实数集 \\(\\mathbb{R}\\) 上所有开区间生成的σ代数，记为 \\(\\mathcal{B}(\\mathbb{R})\\)。</li> </ul> <p>分割（Partition）：若事件 \\(A_1, A_2, \\ldots, A_n\\) 互不相容（即 \\(A_i \\cap A_j = \\varnothing, i \\neq j\\)），且 \\(\\bigcup_{i=1}^{n} A_i = \\Omega\\)，则它们构成 \\(\\Omega\\) 的一个分割。</p>","path":["课程","概率论与统计学","第1章 随机事件与概率"],"tags":[]},{"location":"blog/class/pb_ch1/#5","level":2,"title":"5.概率的发展脉络","text":"<p>发展脉络：</p> <ul> <li>历史时期：古典定义，几何定义，频率定义，主观定义。</li> <li>现代基础：公理化定义（Kolmogorov，苏联）</li> </ul> <p>公理化定义（Kolmogorov）：</p> <p>设 \\(\\Omega\\) 为样本空间，\\(\\mathcal{F}\\) 为事件域，若定义在 \\(\\mathcal{F}\\) 上的集合函数 \\(P: \\mathcal{F} \\to \\mathbb{R}\\) 满足：</p> <p>（1）非负性：\\(\\forall A \\in \\mathcal{F},\\ P(A) \\ge 0\\)；</p> <p>（2）正则性：\\(P(\\Omega) = 1\\)；</p> <p>（3）可列可加性：若 \\(A_1, A_2, \\ldots\\) 两两互斥（\\(A_i \\cap A_j = \\varnothing, i \\neq j\\)），则 \\(P\\left( \\bigcup_{i=1}^{\\infty} A_i \\right) = \\sum_{i=1}^{\\infty} P(A_i)\\)。</p> <p>则称 \\(P\\) 为概率测度，\\(P(A)\\) 为事件 \\(A\\) 的概率。</p> <p>性质（由公理可推导）：</p> <ul> <li>有限可加性：若 \\(A_1, \\ldots, A_n\\) 两两互斥，则 \\(P\\left( \\bigcup_{i=1}^{n} A_i \\right) = \\sum_{i=1}^{n} P(A_i)\\)。</li> <li>单调性：若 \\(A \\subset B\\)，则 \\(P(A) \\le P(B)\\)。</li> <li>加法公式：\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)。对于多个事件有容斥原理。</li> <li>连续性：若事件序列 \\(\\{A_n\\}\\) 单调（如 \\(A_n \\uparrow A\\) 或 \\(A_n \\downarrow A\\)），则 \\(\\lim_{n \\to \\infty} P(A_n) = P(\\lim_{n \\to \\infty} A_n)\\)。</li> </ul> <p>（注：在大多数教科书中频率对于可列可加性的介绍过于模糊。）</p>","path":["课程","概率论与统计学","第1章 随机事件与概率"],"tags":[]},{"location":"blog/class/pb_ch1/#6","level":2,"title":"6.确定概率的方法","text":"<p>频率方法确定概率：</p> <ul> <li>大量重复实验统计频率：\\(P(A) \\approx \\frac{n_A}{n}\\)，其中 \\(n\\) 为总试验次数，\\(n_A\\) 为 \\(A\\) 发生的次数。</li> </ul> <p>古典方法确定概率：</p> <ul> <li>在有限样本的随机现象中，每个样本等可能（\\(\\Omega\\) 中样本点有限且等概）。</li> <li>概率计算公式：\\(P(A) = \\frac{|A|}{|\\Omega|} = \\frac{A\\text{包含的样本点数}}{\\text{样本点总数}}\\)。</li> <li>应用模型：抽样模型（如彩票问题）、盒子模型（如生日问题，统计物理中的模型）。</li> </ul> <p>几何方法确定概率：</p> <ul> <li>适用于样本空间 \\(\\Omega\\) 为可度量的几何区域，且样本点落入子区域 \\(A\\) 的可能性与 \\(A\\) 的测度（长度、面积、体积）成正比，而与形状位置无关。</li> <li>概率计算公式：\\(P(A) = \\frac{\\mu(A)}{\\mu(\\Omega)}\\)，其中 \\(\\mu(\\cdot)\\) 表示几何测度。</li> <li>经典问题：会面问题、蒲丰投针问题（蒙特卡罗方法的雏形）。</li> </ul> <p>主观方法确定概率：</p> <ul> <li>贝叶斯学派观点：概率是主体根据已有经验对事件发生的可能性做出的个人信念度量的判断。</li> <li>应用场景：气象预报，经验总体预估。</li> </ul> <p>（注：关于贝叶斯学派和频率学派的有关文章可见）</p>","path":["课程","概率论与统计学","第1章 随机事件与概率"],"tags":[]},{"location":"blog/class/pb_ch1/#7","level":2,"title":"7.条件概率","text":"<p>定义： 在事件 \\(B\\) 发生（\\(P(B)&gt;0\\)）的条件下，事件 \\(A\\) 发生的概率，记为 \\(P(A|B)\\)。其定义为： $$ P(A|B) = \\frac{P(A \\cap B)}{P(B)} $$</p> <p>实用公式：</p> <ul> <li>乘法公式：\\(P(A \\cap B) = P(B)P(A|B) = P(A)P(B|A)\\)。可推广至多个事件。</li> <li>全概率公式：若 \\(B_1, B_2, \\ldots, B_n\\) 是 \\(\\Omega\\) 的一个分割，且 \\(P(B_i)&gt;0\\)，则对任意事件 \\(A\\) 有： $\\(P(A) = \\sum_{i=1}^{n} P(B_i)P(A|B_i)\\)$</li> <li>贝叶斯公式（逆概率公式）：在全概率公式的条件下，有： </li> </ul> \\[ P(B_j|A) = \\frac{P(B_j)P(A|B_j)}{\\sum_{i=1}^{n} P(B_i)P(A|B_i)},\\quad j=1,2,\\ldots,n \\]","path":["课程","概率论与统计学","第1章 随机事件与概率"],"tags":[]},{"location":"blog/class/pb_ch1/#8","level":2,"title":"8.独立性问题","text":"<ul> <li>两个事件 \\(A\\) 与 \\(B\\) 独立定义为：\\(P(A \\cap B) = P(A)P(B)\\)。当 \\(P(B)&gt;0\\) 时，等价于 \\(P(A|B)=P(A)\\)。</li> <li>多个事件的相互独立与两两独立。</li> </ul> <p>（待续）</p>","path":["课程","概率论与统计学","第1章 随机事件与概率"],"tags":[]},{"location":"blog/class/pb_ch2/","level":1,"title":"第2章 随机变量的分布","text":"<p>随机变量及其分布是概率论的重点，本章主要讨论一维随机变量及其分布。</p>","path":["课程","概率论与统计学","第2章 随机变量的分布"],"tags":[]},{"location":"blog/class/pb_ch2/#1","level":2,"title":"1. 随机变量及其分布函数","text":"<ul> <li> <p>随机变量：定义在样本空间上的实值函数，常用大写字母表示，取值常用小写字母表示。</p> </li> <li> <p>分布函数：对于一个随机变量，定义\\(F(x)=P(X\\leq x)\\)为分布函数，称X服从\\(F(x)\\).满足下列三种基本性质：单调性，有界性，右连续性</p> </li> </ul>","path":["课程","概率论与统计学","第2章 随机变量的分布"],"tags":[]},{"location":"blog/class/pb_ch2/#2","level":2,"title":"2. 随机变量的数学期望","text":"<p>根据分布计算随机变量的特征数（均值，方差，分位数），侧面反映分布的特征。下介绍最重要的特征数：数学期望。</p> <p>数学期望</p> <ul> <li>起源：分赌本问题</li> <li>均值：算术平均，加权平均（根据频率）</li> <li>离散表达式：\\(E(x)=\\sum x_{i}p(x_{i})\\)   (当且仅当绝对收敛的时唯一)</li> <li>连续表达式：\\(E(s)=\\int xp(x) \\, dx\\)</li> <li>物理解释：重心</li> <li>统计学作用：消除随机性的重要手段</li> </ul> <p>数学期望的性质</p> <ul> <li>\\(E(c)=c\\)</li> <li>\\(E(aX)=aE(x)\\)</li> <li>\\(E(g_{1}+g_{2})=E(g_{1})+E(g_{2})\\)</li> </ul>","path":["课程","概率论与统计学","第2章 随机变量的分布"],"tags":[]},{"location":"blog/class/pb_ch2/#3","level":2,"title":"3. 随机变量的方差与标准差","text":"<p>方差</p> <ul> <li>对于随机变量\\(X\\)的均值a，定义新的随机变量\\((x-a)^2\\)，其均值称作方差。</li> <li>记作：\\(Var(X)=E(x-E(x))^2\\)</li> </ul> <p>标准差</p> <ul> <li>定义方差的平方根：\\(\\sqrt{ Var(x) }=\\sigma(X)\\)</li> <li>刻画\\(X\\)的波动程度（集中与分散程度），越小愈集中。</li> </ul> <p>注：</p> <p>（1）标准差的量纲与数学期望一致，标准差的使用倾向更大。</p> <p>（2）期望存在，方差不一定存在。但方差存在，期望一定存在。</p> <p>性质</p> <ul> <li>\\(Var(X)=E(X^2)-[E(x)]^2\\)</li> <li>常数的方差为0</li> <li>\\(Var(aX+b)=a^2Var(X)\\)</li> <li>切比雪夫不等式（chebyshev）： $$ P(\\mid X-E(x)|\\geq \\sigma) \\leq \\frac{Var(X)}{\\sigma^2} $$</li> </ul> <p>注：对于切比雪夫不等式，描述了这样一个现实： 大偏差（\\(X-E(x)\\)）的概率，称作偏差发生概率，其上界与方差成正比，方差愈大，其上界愈大。</p>","path":["课程","概率论与统计学","第2章 随机变量的分布"],"tags":[]},{"location":"blog/class/pb_ch2/#4","level":2,"title":"4. 常用的几种分布（离散）","text":"","path":["课程","概率论与统计学","第2章 随机变量的分布"],"tags":[]},{"location":"blog/class/pb_ch2/#x-sim-bp","level":3,"title":"伯努利分布 \\(X \\sim B(p)\\)","text":"<ul> <li>单次实验的结果（0或1），成功的概率\\(p\\)，也叫0-1分布</li> <li>公式：\\(P(X=k) = p^k (1-p)^{1-k}\\)</li> </ul>","path":["课程","概率论与统计学","第2章 随机变量的分布"],"tags":[]},{"location":"blog/class/pb_ch2/#x-sim-bn-p","level":3,"title":"二项分布 \\(X \\sim B(n, p)\\)","text":"<ul> <li>描述在\\(n\\)次独立重复的伯努利试验中，成功的概率\\(p\\)</li> <li>公式： \\(P(X=k) = C_n^k p^k (1-p)^{n-k}\\)</li> <li>期望与方差：\\(E(X)=np,\\quad Var(X)=np(1-p)\\)</li> </ul>","path":["课程","概率论与统计学","第2章 随机变量的分布"],"tags":[]},{"location":"blog/class/pb_ch2/#x-sim-plambda","level":3,"title":"泊松分布 \\(X \\sim P(\\lambda)\\)","text":"<ul> <li>描述稀有事件在单位时间内发生次数</li> <li>公式\\(P(X=k)=\\frac{\\lambda^k e^{-\\lambda}}{k!}\\)</li> <li>期望与方差：\\(E(X)=\\lambda,\\quad Var(X)=\\lambda\\)</li> </ul>","path":["课程","概率论与统计学","第2章 随机变量的分布"],"tags":[]},{"location":"blog/class/pb_ch2/#x-sim-textgep","level":3,"title":"几何分布 \\(X \\sim \\text{Ge}(p)\\)","text":"<ul> <li>在第\\(k\\)次试验才获得第一次成功的概率</li> <li>公式: \\(P(X=k) = (1-p)^{k-1}p\\)</li> </ul>","path":["课程","概率论与统计学","第2章 随机变量的分布"],"tags":[]},{"location":"blog/class/pb_ch2/#_1","level":3,"title":"超几何分布","text":"<ul> <li>在含有 \\(M\\) 个成功类别的 \\(N\\) 个总体中，不放回地抽取 \\(n\\) 个样本，其中成功类别个数为 \\(k\\)。</li> <li> <p>公式 ： $$ P(X=k) = \\frac{\\binom{M}{k} \\binom{N-M}{n-k}}{\\binom{N}{n}} $$</p> </li> <li> <p>期望与方差： $$   E(X)=n\\cdot\\frac{M}{N},\\quad Var(X)=n\\cdot\\frac{M}{N}\\cdot\\left(1-\\frac{M}{N}\\right)\\cdot\\frac{N-n}{N-1} $$</p> </li> </ul>","path":["课程","概率论与统计学","第2章 随机变量的分布"],"tags":[]},{"location":"blog/class/pb_ch2/#5","level":2,"title":"5. 常用的几种分布（连续）","text":"","path":["课程","概率论与统计学","第2章 随机变量的分布"],"tags":[]},{"location":"blog/class/pb_ch2/#x-sim-nmu-sigma2","level":3,"title":"正态分布 \\(X \\sim N(\\mu, \\sigma^2)\\)","text":"<ul> <li>自然界最常见的分布，描述受众多独立微小因素影响的变量。</li> <li>由均值 \\(\\mu\\) 和方差 \\(\\sigma^2\\)决定，又称高斯分布</li> <li>公式：</li> </ul> \\[ f(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\] <ul> <li>\\(\\hat{\\mu} = \\bar{x}\\)（样本均值），\\(\\hat{\\sigma}^2 = \\frac{1}{n}\\sum(x_i - \\bar{x})^2\\)（样本偏方差）</li> </ul>","path":["课程","概率论与统计学","第2章 随机变量的分布"],"tags":[]},{"location":"blog/class/pb_ch2/#x-sim-ua-b","level":3,"title":"均匀分布 \\(X \\sim U(a, b)\\)","text":"<ul> <li>在区间 \\([a, b]\\) 内，每个数值出现的可能性相等。</li> <li>公式 ：\\(f(x|a, b) = \\frac{1}{b-a}, \\quad a \\le x \\le b\\)</li> <li>期望与方差：\\(E(X)=\\frac{a+b}{2},\\quad Var(X)=\\frac{(b-a)^2}{12}\\)</li> </ul>","path":["课程","概率论与统计学","第2章 随机变量的分布"],"tags":[]},{"location":"blog/class/pb_ch2/#x-sim-textexplambda","level":3,"title":"指数分布 \\(X \\sim \\text{Exp}(\\lambda)\\)","text":"<ul> <li>泊松分布关注单位时间内事件发生了多少次，指数分布则关注两次事件发生的间隔时间。（无记忆性）</li> <li>概率密度函数：\\(f(x)=\\lambda e^{-\\lambda x},\\quad x\\ge0\\)</li> <li>累积分布函数：\\(F(x|\\lambda) = 1 - e^{-\\lambda x}, \\quad x \\ge 0\\)</li> <li>均值/期望：\\(E(X) = \\frac{1}{\\lambda}\\)</li> <li>方差：\\(Var(X) = \\frac{1}{\\lambda^2}\\)</li> </ul>","path":["课程","概率论与统计学","第2章 随机变量的分布"],"tags":[]},{"location":"blog/class/pb_ch2/#x-sim-gammaalpha-beta","level":3,"title":"伽马分布  \\(X \\sim \\Gamma(\\alpha, \\beta)\\)","text":"<ul> <li>指数分布的推广，如果指数分布是“等 1 次随机事件发生的时间”，那么伽马分布就是“等 \\(\\alpha\\) 次事件发生的时间”。</li> <li>概率密度函数：\\(f(x)=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x},\\quad x&gt;0\\)</li> <li>期望与方差：  \\(E(X)=\\frac{\\alpha}{\\beta},\\quad Var(X)=\\frac{\\alpha}{\\beta^2}\\)</li> <li>具有加性：如果 \\(X_1, X_2\\) 独立且服从相同 \\(\\beta\\) 的伽马分布，其和仍服从伽马分布。</li> <li>当 \\(\\alpha = 1\\) 时，退化为指数分布。</li> <li>当 \\(\\alpha = \\frac{n}{2}, \\beta = \\frac{1}{2}\\) 时，退化为卡方分布。</li> </ul>","path":["课程","概率论与统计学","第2章 随机变量的分布"],"tags":[]},{"location":"blog/class/pb_ch2/#x-sim-textbealpha-beta","level":3,"title":"贝塔分布 \\(X \\sim \\text{Be}(\\alpha, \\beta)\\)","text":"<ul> <li>定义在区间 \\([0, 1]\\) 上的连续分布。它最常用于建模概率的概率。在贝叶斯估计中，它是二项分布的共轭先验。</li> <li>参数：\\(\\alpha, \\beta\\) 控制分布的偏斜程度和峰度。</li> <li>当 \\(\\alpha = 1, \\beta = 1\\) 时，它就是 \\([0, 1]\\) 上的均匀分布。</li> <li>如果 \\(\\alpha = \\beta\\)，分布是对称的。</li> </ul>","path":["课程","概率论与统计学","第2章 随机变量的分布"],"tags":[]},{"location":"blog/class/pb_ch2/#x-sim-chi2n","level":3,"title":"卡方分布 \\(X \\sim \\chi^2(n)\\)","text":"<ul> <li>是 k 个独立标准正态随机变量的平方和所服从的分布，其中k为自由度。</li> <li>概率密度函数：</li> </ul> \\[ f(x)=\\frac{1}{2^{k/2}\\Gamma(k/2)}x^{k/2-1}e^{-x/2},\\quad x&gt;0 \\] <ul> <li>期望与方差  ：\\(E(X)=k,\\quad Var(X)=2k\\)</li> <li>分布始终在正轴上。</li> <li>随着自由度 \\(n\\) 的增大，分布逐渐趋近于正态分布。</li> <li>用于检验方差</li> </ul>","path":["课程","概率论与统计学","第2章 随机变量的分布"],"tags":[]},{"location":"blog/class/pb_ch2/#6","level":2,"title":"6. 分布的其他特征数","text":"","path":["课程","概率论与统计学","第2章 随机变量的分布"],"tags":[]},{"location":"blog/class/pb_ch2/#k","level":4,"title":"\\(k\\)阶原点矩","text":"<ul> <li>\\(\\mu_{k}= E(X^k)\\)</li> <li>一阶原点矩：数学期望</li> </ul>","path":["课程","概率论与统计学","第2章 随机变量的分布"],"tags":[]},{"location":"blog/class/pb_ch2/#k_1","level":4,"title":"\\(k\\)阶中心矩","text":"<ul> <li>\\(\\nu_{k}= E(X-E(X))^k\\)</li> <li>二阶中心矩：方差</li> </ul> <p>二者存在对应关系</p> <ul> <li>变异系数：标准差比期望（消除量纲的影响）</li> <li>分位数：分为两块（下侧，上侧）</li> <li>中位数：均分的分位数</li> <li>偏度系数：描述分布偏离对称性的程度（三阶中心矩/二阶中心矩的3/2次方）</li> <li>峰度系数：描述分布尖峭程度或尾部粗细（四阶中心矩/二阶中心矩的2次方 -3）</li> </ul>","path":["课程","概率论与统计学","第2章 随机变量的分布"],"tags":[]},{"location":"blog/class/pb_intro/","level":1,"title":"绪论","text":"<p>随机变量及其分布是概率论的重点，本章主要讨论一维随机变量及其分布。</p>","path":["课程","概率论与统计学","绪论"],"tags":[]},{"location":"blog/class/pb_intro/#1","level":4,"title":"1. 随机变量及其分布函数","text":"<p>随机变量：定义在样本空间上的实值函数，常用大写字母表示，取值常用小写字母表示。</p> <p>分布函数：对于一个随机变量，定义\\(F(x)=P(X\\leq x)\\)为分布函数，称X服从\\(F(x)\\).满足下列三种基本性质： - 单调性 - 有界性 - 右连续性</p> <p>离散随机变量：分布列（非负性，正则性）</p> <p>连续随机变量：密度函数（非负性，正则性）</p>","path":["课程","概率论与统计学","绪论"],"tags":[]},{"location":"blog/class/pb_intro/#2","level":4,"title":"2. 随机变量的数学期望","text":"<p>根据分布计算随机变量的特征数（均值，方差，分位数），侧面反映分布的特征。下介绍最重要的特征数：数学期望。</p> <p>起源：分赌本问题 均值：算术平均，加权平均（根据频率） 离散表达式：\\(E(x)=\\sum x_{i}p(x_{i})\\)   (当且仅当绝对收敛的时候，此时唯一) 连续表达式：\\(E(s)=\\int xp(x) \\, dx\\) 物理解释：重心 统计学作用：消除随机性的重要手段</p> <p>数学期望的性质： \\(E(c)=c\\) \\(E(aX)=aE(x)\\) \\(E(g_{1}+g_{2})=E(g_{1})+E(g_{2})\\)</p>","path":["课程","概率论与统计学","绪论"],"tags":[]},{"location":"blog/class/pb_intro/#3","level":4,"title":"3. 随机变量的方差与标准差","text":"<p>方差： 对于随机变量\\(X\\)的均值a，定义新的随机变量\\((x-a)^2\\)，其均值称作方差。 \\(Var(X)=E(x-E(x))^2\\)</p> <p>标准差： 定义方差的平方根：\\(\\sqrt{ Var(x) }=\\sigma(X)\\)</p> <p>作用：都刻画\\(X\\)的波动程度（集中与分散程度）。方差/标准差越小，取值愈集中。</p> <p>注：由于标准差的量纲与数学期望一致，标准差的使用倾向更大。期望存在，方差不一定存在。但方差存在，期望一定存在。</p> <p>性质*： 1.\\(Var(X)=E(X^2)-[E(x)]^2\\) 2.常数的方差为0 3.\\(Var(aX+b)=a^2Var(X)\\) 4.切比雪夫不等式（chebyshev）： $$ P(\\mid X-E(x)|\\geq \\sigma) \\leq \\frac{Var(X)}{\\sigma^2} $$  对于切比雪夫不等式，描述了这样一个现实：  大偏差（\\(X-E(x)\\)）的概率，称作偏差发生概率，其上界与方差成正比，方差愈大，其上界愈大。</p>","path":["课程","概率论与统计学","绪论"],"tags":[]},{"location":"blog/class/pb_intro/#4","level":4,"title":"4. 常用的几种分布（离散）","text":"<p>二项分布   定义：描述在 \\( n \\) 次独立重复的伯努利试验中，成功次数 \\( X \\) 的概率分布。   概率质量函数$\\(P(X=k) = C_n^k p^k (1-p)^{n-k}, \\quad k=0,1,\\dots,n\\)$期望与方差：  $\\(E(X)=np,\\quad Var(X)=np(1-p)\\)$应用：适用于结果只有两种（成功/失败）的重复独立试验。</p> <p>泊松分布   定义：描述在单位时间（或空间）内，随机事件发生次数的概率分布。   概率质量函数：  $$   P(X=k)=\\frac{\\lambda^k e^{-\\lambda}}{k!}, \\quad k=0,1,2,\\dots$\\(**期望与方差**：\\)\\(E(X)=\\lambda,\\quad Var(X)=\\lambda\\)$应用：稀有事在一定时间或区间内发生次数的建模。</p> <p>关系：泊松分布可以作为二项分布的近似（泊松定理：当 \\(n\\) 很大、\\(p\\)  很小，且 \\(np\\) 适中时，泊松分布可作为二项分布的近似 \\(\\lambda = np\\) ）</p> <p>超几何分布  定义：描述在不放回抽样中，从包含 \\( M \\) 个成功个体和 \\( N-M \\) 个失败个体的总体中抽取 \\( n \\) 个个体，其中成功个体数 \\( X \\) 的分布。  概率质量函数：  $$   P(X=k)=\\frac{CM^k C<sup>{n-k}}{C_N</sup>n}$$期望与方差：  $$   E(X)=n\\cdot\\frac{M}{N},\\quad Var(X)=n\\cdot\\frac{M}{N}\\cdot\\left(1-\\frac{M}{N}\\right)\\cdot\\frac{N-n}{N-1}$$应用：适用于不放回抽样的质量控制或抽样调查。</p> <p>几何分布   定义：描述在多次独立伯努利试验中，首次成功所需试验次数 \\( X \\) 的概率分布。   概率质量函数：  $\\(P(X=k)=(1-p)^{k-1}p,\\quad k=1,2,\\dots\\)$期望与方差：  $$  E(X)=\\frac{1}{p},\\quad Var(X)=\\frac{1-p}{p^2}$$应用：用于建模在多次尝试中首次获得成功所需的次数。</p> <p>负二项分布   定义：描述在独立重复试验中，获得第 \\( r \\) 次成功所需试验次数 \\( X \\) 的概率分布。   概率质量函数：  $\\(P(X=k)=C_{k-1}^{r-1}p^r(1-p)^{k-r},\\quad k=r,r+1,\\dots\\)\\(**期望与方差**：\\)\\(E(X)=\\frac{r}{p},\\quad Var(X)=\\frac{r(1-p)}{p^2}\\)$应用：适用于需要多次成功的情境，如多次试验直到达成特定目标。</p>","path":["课程","概率论与统计学","绪论"],"tags":[]},{"location":"blog/class/pb_intro/#5","level":4,"title":"5. 常用的几种分布（连续）","text":"<p>正态分布   定义：又称高斯分布，是连续型概率分布中最重要的一种，由均值 \\(\\mu\\) 和方差 \\(\\sigma^2\\)决定。   概率密度函数：  $\\(f(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}},\\quad -\\infty&lt;x&lt;\\infty\\)$期望与方差：  $\\(E(X)=\\mu,\\quad Var(X)=\\sigma^2\\)$ 应用：广泛应用于自然科学和社会科学中的数据建模。</p> <p>均匀分布   定义：在区间 \\([a,b]\\) 内，每个点出现的概率相等的分布。   概率密度函数：  $$f(x)=\\begin{cases}   \\frac{1}{b-a}, &amp; a\\le x\\le b \\   0, &amp; \\text{otherwise}   \\end{cases} $$期望与方差：  $$   E(X)=\\frac{a+b}{2},\\quad Var(X)=\\frac{(b-a)^2}{12}$$应用：适用于在某个区间内等可能取值的随机现象。</p> <p>指数分布   定义：描述泊松过程中事件发生时间间隔的概率分布。   概率密度函数：  $\\(f(x)=\\lambda e^{-\\lambda x},\\quad x\\ge0\\)$期望与方差：  $\\(E(X)=\\frac{1}{\\lambda},\\quad Var(X)=\\frac{1}{\\lambda^2}\\)$应用：常用于可靠性分析和排队论，描述无记忆性的随机时间间隔。</p> <p>伽马分布   定义：指数分布的推广，描述多个独立指数分布随机变量之和的分布。   概率密度函数：  $$ f(x)=\\frac{\\beta<sup>\\alpha}{\\Gamma(\\alpha)}x</sup>,\\quad x&gt;0$$}e^{-\\beta x期望与方差：  $$   E(X)=\\frac{\\alpha}{\\beta},\\quad Var(X)=\\frac{\\alpha}{\\beta^2} $$应用：适用于对连续正随机变量建模，如等待时间的总和。</p> <p>贝塔分布   定义：定义在区间 \\([0,1]\\) 上的连续概率分布，由两个正参数 \\(\\alpha\\) 和 \\(\\beta\\)  决定。   概率密度函数：  $\\(f(x)=\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha,\\beta)},\\quad 0\\le x\\le1\\)$期望与方差：  $\\(E(X)=\\frac{\\alpha}{\\alpha+\\beta},\\quad Var(X)=\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\)$应用：常用于表示概率的概率分布，适用于比例或百分比的建模。</p> <p>卡方分布   定义：是 k 个独立标准正态随机变量的平方和所服从的分布，其中k为自由度。   概率密度函数：  $\\(f(x)=\\frac{1}{2^{k/2}\\Gamma(k/2)}x^{k/2-1}e^{-x/2},\\quad x&gt;0\\)$期望与方差：  $$ E(X)=k,\\quad Var(X)=2k$$应用：主要用于假设检验和置信区间的构建，特别是方差分析和拟合优度检验。</p>","path":["课程","概率论与统计学","绪论"],"tags":[]},{"location":"blog/class/pb_intro/#6","level":4,"title":"6. 分布的其他特征数","text":"<p>\\(k\\)阶原点矩 - \\(\\mu_{k}= E(X^k)\\) - 一阶原点矩：数学期望</p> <p>\\(k\\)阶中心矩 - \\(\\nu_{k}= E(X-E(X))^k\\) - 二阶中心矩：方差</p> <p>二者存在对应关系</p> <p>变异系数：标准差比期望（消除量纲的影响）</p> <p>分位数：分为两块（下侧，上侧）</p> <p>中位数：均分的分位数</p> <p>偏度系数：描述分布偏离对称性的程度（三阶中心矩/二阶中心矩的3/2次方）</p> <p>峰度系数：描述分布尖峭程度或尾部粗细（四阶中心矩/二阶中心矩的2次方 -3）</p>","path":["课程","概率论与统计学","绪论"],"tags":[]},{"location":"blog/class/pb_question/","level":1,"title":"问答","text":"<p>Question</p> <p>概率论和统计学的区别是什么？</p> <p>概率论是对已有的数据分析，得到其特征和规律。 统计学是根据已有的结果，推断背后的规律。</p> <p>Question</p> <p>能否给出一个能串联的脉络？</p> <p>概率论：公理化（定义概率）——大数定理（均值收敛/期望）——中心极限定理（分布收敛/归一化）</p> <p>统计学：现实数据（描述）——抽样分布理论（特征）——参数估计（估计）——假设检验（决策）</p> <p>Question</p> <p>说明一下大数定理？</p> <p>大数定理指出：样本均值趋于总体均值（真实期望） 关心的是样本均值的收敛性（这个点依概率收敛到总体期望） 提供了稳定性。</p> <p>Question</p> <p>说明一下中心极限定理？</p> <p>中心极限定理：独立同分布的随机变量趋于正态。 关心的是样本均值的分布形状（标准化后收敛到正态分布） 为估计提供了量化工具来计算概率。</p> <p>Question</p> <p>如何进行参数估计，或者说方法？</p> <p>对于点估计，主要最大似然估计、矩估计。 对于区间估计，抽样分布理论 t分布，f分布，正态分布，卡方分布</p> <p>Question</p> <p>如何判断一个估计量的好坏？</p> <p>无偏性，有效性，一致性。</p> <p>Question</p> <p>有哪些补充与扩展？</p> <p>比如公理化定义后，后续证明的基石，导向了均值（大数定理）和分布（中心极限定理）的马尔可夫不等式，切比雪夫不等式。</p> <p>比如估计的方法上，主要包括最大似然估计，矩估计。</p> <p>Question</p> <p>对于概率的理解存在的哲学之争？</p> <p>长期以来，频率学派一直占据重要的主导地位，但随着计算机的兴起，贝叶斯学派接受的人越来越多。 贝叶斯学派：概率是对不确定性的主观信任程度，认为参数是随机的。 传统频率学派：概率是大量实验中的比例（频率）。</p> <p>Question</p> <p>概率论在现代领域的应用？</p> <p>在数据科学和机器学习上，概率统计作为理论的基石，不论思想还是方法上都是处理的重要工具。</p> <p>Question</p> <p>后续的发展</p> <p>（1）随着已有的基础理论不断完善，对现实世界动态波动的概率的研究成为主流，建立了随机过程这一基础上的领域。</p> <p>马尔可夫过程（未来只依赖现在）——鞅论——随机分析 泊松过程 布朗运动</p> <p>（3）面向认知与信息</p> <p>信息论：熵，互信息，KL散度。</p>","path":["课程","概率论与统计学","问答"],"tags":[]},{"location":"blog/essay/huiyilu/","level":1,"title":"Huiyilu","text":"<p>啊啊</p>","path":["随笔","Huiyilu"],"tags":[]},{"location":"blog/posts/2025-01-22-hello-world/","level":1,"title":"我的第一篇博客","text":"<p>这是文章内容...</p>","path":["我的第一篇博客"],"tags":["数学","机器学习"]},{"location":"blog/posts/links_ai/","level":1,"title":"AI网站导航","text":"","path":["档案","AI网站导航"],"tags":[]},{"location":"blog/posts/links_ai/#_1","level":2,"title":"综合平台","text":"<ul> <li>@Kaggle - 数据科学竞赛与社区平台</li> <li>@Hugging Face - 开源模型与数据集平台</li> <li>@Papers with Code - 论文与代码集合</li> </ul>","path":["档案","AI网站导航"],"tags":[]},{"location":"blog/posts/links_ai/#ai_1","level":2,"title":"AI模型与工具","text":"<ul> <li>@OpenAI - ChatGPT、GPT系列模型</li> <li>@Anthropic Claude - Claude智能助手</li> <li>@Google Gemini - Google AI助手</li> <li>@Midjourney - AI图像生成</li> <li>@Stable Diffusion - 开源图像生成模型</li> <li>@DeepSeek - 深度求索AI助手</li> </ul>","path":["档案","AI网站导航"],"tags":[]},{"location":"blog/posts/links_ai/#_2","level":2,"title":"开发与代码","text":"<ul> <li>@GitHub Copilot - AI编程助手</li> <li>@Replit - 在线AI编程环境</li> <li>@Cursor - AI代码编辑器</li> </ul>","path":["档案","AI网站导航"],"tags":[]},{"location":"blog/posts/links_ai/#_3","level":2,"title":"搜索与学习","text":"<ul> <li>@Perplexity - AI搜索引擎</li> <li>@arXiv - 学术论文预印本</li> <li>@Google AI - Google AI研究</li> </ul>","path":["档案","AI网站导航"],"tags":[]},{"location":"blog/posts/links_book/","level":1,"title":"书籍汇总","text":"<p>书籍</p>","path":["书籍汇总"],"tags":[]},{"location":"blog/posts/links_math/","level":1,"title":"一些数学的网站","text":"<ul> <li>@刘思齐老师的主页 - 包含录课视频 </li> <li>@Math Stack Exchange - 数学问答社区</li> <li>@李文威老师的主页 - 代数学方法</li> <li>@谢启鸿老师的博客 - 高等代数方面</li> <li>@香蕉空间 - 类wiki的知识库</li> <li>@Overleaf - 在线LaTeX编辑平台</li> <li>@Beamer主题矩阵 - Beamer主题预览与配色</li> </ul>","path":["档案","一些数学的网站"],"tags":[]},{"location":"blog/posts/seminar/","level":1,"title":"Seminar & Project","text":"","path":["Seminar & Project"],"tags":[]},{"location":"blog/posts/seminar/#_1","level":2,"title":"研讨会","text":"<p>待续……</p>","path":["Seminar & Project"],"tags":[]},{"location":"blog/posts/seminar/#_2","level":2,"title":"项目","text":"<p>数学和生物学类：</p> <ul> <li>一类生态学复杂动力系统涌现现象的研究</li> </ul> <p>代数学和密码学类：</p> <ul> <li>基于有理变换的一些不可约多项式的构造</li> </ul> <p>机器学习类：</p> <ul> <li> <p>肝脏与胰腺医学影像中的人工智能应用</p> </li> <li> <p>基于SIFT算法的特征点图像拼接</p> </li> </ul>","path":["Seminar & Project"],"tags":[]}]}